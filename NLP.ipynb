{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BraedynL0530/PortfolioWebsite/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ7xNvJ17NHf"
      },
      "outputs": [],
      "source": [
        "# README Summary Generator - FIXED VERSION\n",
        "# Key fixes: Progress visibility, faster model, better error handling\n",
        "\n",
        "# SETUP\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q transformers torch accelerate safetensors\n",
        "\n",
        "# CONFIGURATION\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Force output flush for Colab\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Paths\n",
        "DRIVE_BASE = '/content/drive/MyDrive/readme_training'\n",
        "INPUT_FILE = f'{DRIVE_BASE}/training_data.json'\n",
        "CHECKPOINT_FILE = f'{DRIVE_BASE}/summaries_checkpoint.json'\n",
        "OUTPUT_FILE = f'{DRIVE_BASE}/summaries_final.json'\n",
        "\n",
        "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
        "\n",
        "# CRITICAL: Use a smaller, faster model for Colab\n",
        "# Pick ONE (uncomment it):\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # RECOMMENDED: Fast, no auth\n",
        "# MODEL_NAME = \"microsoft/phi-2\"  # Good quality, medium speed\n",
        "# MODEL_NAME = \"google/flan-t5-base\"  # Very fast, different architecture\n",
        "# MODEL_NAME = \"TheBloke/vicuna-7B-1.1-HF\"  # Slow but higher quality\n",
        "\n",
        "BATCH_SIZE = 1  # Process one at a time for better progress tracking\n",
        "CHECKPOINT_INTERVAL = 5  # Save every 5 summaries\n",
        "\n",
        "print(f\"âœ… Model: {MODEL_NAME}\")\n",
        "print(f\"âœ… Checkpoint every: {CHECKPOINT_INTERVAL}\")\n",
        "print(f\"âœ… Drive path: {DRIVE_BASE}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# LOAD MODEL\n",
        "print(\"\\nðŸ“¦ Loading model... (this may take 2-5 minutes)\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"âœ… Model loaded successfully!\")\n",
        "    print(f\"âœ… Using device: {model.device}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ MODEL LOAD FAILED: {e}\")\n",
        "    print(\"Try using: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    sys.stdout.flush()\n",
        "    raise\n",
        "\n",
        "# LOAD DATA\n",
        "print(\"\\nðŸ“‚ Loading README data...\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "with open(INPUT_FILE, 'r') as f:\n",
        "    readmes_data = json.load(f)\n",
        "\n",
        "print(f\"âœ… Loaded {len(readmes_data)} READMEs\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Load checkpoint\n",
        "try:\n",
        "    with open(CHECKPOINT_FILE, 'r') as f:\n",
        "        processed_summaries = json.load(f)\n",
        "    processed_indices = {s['id'] for s in processed_summaries}\n",
        "    print(f\"ðŸ“‹ Resuming: {len(processed_summaries)} already done\")\n",
        "except FileNotFoundError:\n",
        "    processed_summaries = []\n",
        "    processed_indices = set()\n",
        "    print(\"ðŸ“‹ Starting fresh\")\n",
        "\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Add unique IDs if missing\n",
        "for i, item in enumerate(readmes_data):\n",
        "    if 'id' not in item:\n",
        "        item['id'] = i\n",
        "\n",
        "remaining = [r for r in readmes_data if r['id'] not in processed_indices]\n",
        "print(f\"ðŸ“Š Remaining: {len(remaining)}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# SUMMARY GENERATOR\n",
        "def generate_summary(readme_text, max_length=1500):\n",
        "    \"\"\"Generate summary with better error handling\"\"\"\n",
        "\n",
        "    # Truncate\n",
        "    if len(readme_text) > max_length:\n",
        "        readme_text = readme_text[:max_length] + \"...\"\n",
        "\n",
        "    # Improved prompt for better technical summaries\n",
        "    prompt = f\"\"\"Write a technical summary of this GitHub project as a single paragraph with 3-4 complete sentences.\n",
        "\n",
        "Your summary should cover:\n",
        "- What the project does and its main purpose\n",
        "- Technologies used (programming languages, frameworks, libraries) - only mention what's in the README\n",
        "- Key features or implementation details\n",
        "\n",
        "Do NOT use numbered lists or bullet points. Write in natural paragraph form.\n",
        "\n",
        "README:\n",
        "{readme_text}\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,  # Reduced for speed\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract summary - now looking for \"Technical Summary:\"\n",
        "        if \"Technical Summary:\" in full_output:\n",
        "            summary = full_output.split(\"Technical Summary:\")[-1].strip()\n",
        "        elif \"Summary:\" in full_output:\n",
        "            summary = full_output.split(\"Summary:\")[-1].strip()\n",
        "        else:\n",
        "            summary = full_output[len(prompt):].strip()\n",
        "\n",
        "        # Clean up\n",
        "        summary = summary.replace('\\n', ' ').strip()\n",
        "\n",
        "        # Remove any trailing incomplete sentences (but keep full summary)\n",
        "        # Only trim if it's unreasonably long (over 1000 chars)\n",
        "        if len(summary) > 1000:\n",
        "            # Try to cut at last sentence\n",
        "            last_period = summary[:1000].rfind('.')\n",
        "            if last_period > 500:\n",
        "                summary = summary[:last_period + 1]\n",
        "\n",
        "        return summary\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Generation error: {e}\")\n",
        "        return f\"Error generating summary: {str(e)[:100]}\"\n",
        "\n",
        "# MAIN PROCESSING LOOP\n",
        "print(\"\\nðŸš€ Starting generation...\\n\")\n",
        "print(\"=\" * 60)\n",
        "sys.stdout.flush()\n",
        "\n",
        "for i, readme_data in enumerate(remaining):\n",
        "    try:\n",
        "        print(f\"\\n[{i+1}/{len(remaining)}] Processing: {readme_data.get('repo_name', 'Unknown')}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Generate\n",
        "        summary = generate_summary(readme_data['readme'])\n",
        "\n",
        "        # Save result\n",
        "        readme_data['summary'] = summary\n",
        "        processed_summaries.append(readme_data)\n",
        "\n",
        "        # Show FULL summary for first 5, then preview for rest\n",
        "        if i < 5:\n",
        "            print(f\"   âœ“ FULL: {summary}\")\n",
        "        else:\n",
        "            preview = summary[:100] + \"...\" if len(summary) > 100 else summary\n",
        "            print(f\"   âœ“ {preview}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Checkpoint\n",
        "        if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
        "            with open(CHECKPOINT_FILE, 'w') as f:\n",
        "                json.dump(processed_summaries, f, indent=2)\n",
        "            print(f\"\\nðŸ’¾ CHECKPOINT SAVED: {len(processed_summaries)} summaries\")\n",
        "            print(\"=\" * 60)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ FAILED {readme_data.get('repo_name', 'Unknown')}: {e}\")\n",
        "        sys.stdout.flush()\n",
        "        continue\n",
        "\n",
        "# FINAL SAVE\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ’¾ Saving final results...\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "with open(CHECKPOINT_FILE, 'w') as f:\n",
        "    json.dump(processed_summaries, f, indent=2)\n",
        "\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    json.dump(processed_summaries, f, indent=2)\n",
        "\n",
        "print(f\"\"\"\n",
        "âœ¨ COMPLETE! âœ¨\n",
        "\n",
        "ðŸ“Š Stats:\n",
        "   Total: {len(processed_summaries)} summaries\n",
        "   Checkpoint: {CHECKPOINT_FILE}\n",
        "   Final: {OUTPUT_FILE}\n",
        "\n",
        "ðŸŽ¯ Next: Download from Google Drive and train your model!\n",
        "\"\"\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# PREVIEW\n",
        "print(\"\\nðŸ“‹ Sample summaries:\")\n",
        "for i, item in enumerate(processed_summaries[:3]):\n",
        "    print(f\"\\n{i+1}. {item.get('repo_name', 'Unknown')} ({item.get('stars', 0)} â­)\")\n",
        "    print(f\"   {item['summary']}\")\n",
        "sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "unIvygqcFwyc"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import forward_ad\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import  dataclass\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#config\n",
        "@dataclass\n",
        "class config:\n",
        "  vocab_size: int #unique words\n",
        "  block_size: int #how far back(context) it can see, memory/ how many tokens back\n",
        "  n_layer: int   # stacked blocks, more layers more reasoning more train time\n",
        "  n_head: int   # attentions per layer, how many \"eyes\" looking for a new pattern\n",
        "  n_embd: int   #size of vector for each token\n",
        "  dropout: float   #prevents overfitting by stopping random paths\n",
        "  pad_token_id: int = 50256\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "self attention: part 1 of transformer\n",
        "Q K V, query key value. helps use the two embeddings to learn diffrent meanings for words and give the diffrent vectors even if the same word\n",
        "below is theory class is optimized, it condences the prjections into one huge vector and splits. other than that its nearly identical just more efficent\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "#learnable compenets\n",
        "q_prog = nn.Linear(C, C, bias =False)\n",
        "k_prog = nn.Linear(C, C, bias =False)\n",
        "v_prog = nn.Linear(C, C, bias =False)\n",
        "\n",
        "#weights\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "\n",
        "#preform projection\n",
        "q = q_prog(x)\n",
        "k = k_prog(x)\n",
        "v = v_prog(x)\n",
        "\n",
        "scores = q @ k.transpose(-2,-1)\n",
        "print(\"scores\",scores)\n",
        "\n",
        "\n",
        "\n",
        "Attention(Q,K,V)=softmax(â€‹QK^âŠ¤/dkâ€‹â€‹)V\n",
        "\n",
        "d_k = k.size(-1)#last dimesion of\n",
        "scaled_scores = scores / math.sqrt(d_k)\n",
        "attention_weights = F.softmax(scaled_scores, dim=1)\n",
        "print(\"scaled scores\", scaled_scores)\n",
        "print(\"scaled scores -> percentages\", attention_weights)\n",
        "\n",
        "# aggreation Last part of attention!\n",
        "output = attention_weights @ v\n",
        "print(\"output!:\",output)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Core logic for MultiHead\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config :config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False) # Fuzed layer = more efficent\n",
        "\n",
        "    self.register_buffer( # part of causal masking\n",
        "        \"bias\",# buffer name\n",
        "        torch.tril(torch.ones(config.block_size,config.block_size))\n",
        "        .view(1,1, config.block_size, config.block_size)\n",
        "    )\n",
        "\n",
        "    self.c_proj = nn.Linear(config.n_embd,config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()\n",
        "    head_dim = C // self.n_head\n",
        "\n",
        "    # project once -> split\n",
        "    qkv = self.c_attn(x)\n",
        "    q, k, v = qkv.split(C, dim=2)\n",
        "\n",
        "    # reshape into heads\n",
        "    q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "    # attention\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_dim))\n",
        "    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\")) # prevents it from seeing future tokens\n",
        "    att = F.softmax(att, dim=-1)\n",
        "\n",
        "    # aggregate :3\n",
        "    y = att @ v\n",
        "\n",
        "    # merge heads\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "    # final projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config :config):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(config.n_embd, 4 * config.n_embd) #expands dimestions, think of it as more room to think / combining features\n",
        "    self.proj = nn.Linear(4 * config.n_embd, config.n_embd) # condenses back so it can be added back to attetion\n",
        "    self.drop = nn.Dropout(config.dropout) #refer to config\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = F.gelu(x) # makes x nonlinear so fc and proj dont just merge into one straight line\n",
        "    x =self.proj(x)\n",
        "    x = self.drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module): #residual connection\n",
        "  def __init__(self, config : config): #litterly just does f(x) + x instead of f(x) so mlp dosesnt relearn it takes the learned/trained data and keeps it\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # focus (the \"+\")\n",
        "    x = x +self.attn(self.ln_1(x))\n",
        "\n",
        "    x = x +self.mlp(self.ln_2(x))\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d9MJ7SDf9p3Y"
      },
      "outputs": [],
      "source": [
        "class NLP(nn.Module):\n",
        "  def __init__(self, config: config):\n",
        "    super().__init__()\n",
        "    # Input\n",
        "    self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "    self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
        "    self.drop = nn.Dropout(config.dropout)\n",
        "    self.config = config\n",
        "    self.pad_token_id = config.pad_token_id\n",
        "\n",
        "\n",
        "\n",
        "    # Processing, makes a stack/block / LAYER for deeper understanding\n",
        "    # Data flows through sequncesnsy so more refined/better understanding\n",
        "    self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "\n",
        "    #output layers\n",
        "    self.ln_f = nn.LayerNorm(config.n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False) #language model head, parrel prediction(linear) makes raw score for each possible next token , good for training, and throws away the\n",
        "    #rest(all but last vector) if not traning\n",
        "    # Above makes raw score for each possible next token\n",
        "\n",
        "\n",
        "    self.lm_head.weight = self.wte.weight\n",
        "\n",
        "  def forward(self, idx, targets= None): #idx = input targets = inputs shifter one pos to left\n",
        "    B, T = idx.size()\n",
        "\n",
        "    assert T <= self.config.block_size, f\"Sequence length {T} exceeds block_size {self.config.block_size}\"\n",
        "\n",
        "    tok_emb = self.wte(idx)\n",
        "    pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "    pos_emb = self.wpe(pos)\n",
        "    x = self.drop(tok_emb + pos_emb)\n",
        "\n",
        "    # Process through transformer blocks\n",
        "    for block in self.h:\n",
        "        x = block(x)\n",
        "\n",
        "    # Final layer norm\n",
        "    x = self.ln_f(x)\n",
        "\n",
        "    # Output logits\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      print(f\"    Logits shape: {logits.shape}\")\n",
        "      print(f\"    Targets shape: {targets.shape}\")\n",
        "      print(f\"    Logits min/max: {logits.min().item():.4f}/{logits.max().item():.4f}\")\n",
        "      print(f\"    Targets min/max: {targets.min().item()}/{targets.max().item()}\")\n",
        "\n",
        "\n",
        "      if targets is not None:\n",
        "        # Shift for training\n",
        "        logits_shifted = logits[:, :-1, :]\n",
        "        targets_shifted = targets[:, 1:]\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            logits_shifted.reshape(-1, logits_shifted.size(-1)), #flattens to 2d and gets loss\n",
        "            targets_shifted.reshape(-1),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "\n",
        "      print(f\"    Computed loss: {loss.item():.6f}\")\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def summarize(self, idx, max_new_tokens, tempature = 1.0, top_k =0): # <1 rare words more likely, >1 more common words\n",
        "    # Crop if too long\n",
        "    if idx.size(1) > self.config.block_size:\n",
        "      idx_cond = idx[:, -self.config.block_size:]\n",
        "    else:\n",
        "      idx_cond = idx\n",
        "\n",
        "    logits, _ = self(idx_cond)\n",
        "    logits = logits[:, -1, :] / tempature  # Get last token predictions\n",
        "\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)  # Fixed typo\n",
        "\n",
        "    idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHZ3z6SdsnYA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-PUEltGnIF5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "#device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open(OUTPUT_FILE) as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer vocab_size: {len(tokenizer)}\")  # Should print 50257\n",
        "\n",
        "config_instance = config(\n",
        "    vocab_size=len(tokenizer),  # This MUST be 50257\n",
        "    block_size=1024,\n",
        "    n_layer=6,\n",
        "    n_head=12,\n",
        "    n_embd=768,\n",
        "    dropout=0.2,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "print(f\"Config vocab_size: {config_instance.vocab_size}\")  # Verify it's 50257\n",
        "\n",
        "model = NLP(config_instance).to(device)\n",
        "\n",
        "print(f\"Model wte shape: {model.wte.weight.shape}\")  # Should be (50257, 768)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 3\n",
        "IGNORE_INDEX = -100\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    for i in range(0, len(data), BATCH_SIZE):\n",
        "        batch = data[i:i+BATCH_SIZE]\n",
        "        input_ids = []\n",
        "        target_ids = []\n",
        "\n",
        "        for item in batch:\n",
        "            readme_tokens = tokenizer.encode(item[\"readme\"], max_length=768)\n",
        "            summary_tokens = tokenizer.encode(\" SUMMARY: \" + item[\"summary\"], max_length=256)\n",
        "\n",
        "            tokens = (readme_tokens + summary_tokens)[:1024]\n",
        "            targets = [IGNORE_INDEX] * len(readme_tokens) + summary_tokens\n",
        "            targets = targets[:1024]\n",
        "\n",
        "            pad_len = 1024 - len(tokens)\n",
        "            if pad_len > 0:\n",
        "                tokens += [tokenizer.pad_token_id] * pad_len\n",
        "                targets += [IGNORE_INDEX] * pad_len\n",
        "\n",
        "            input_ids.append(tokens)\n",
        "            target_ids.append(targets)\n",
        "\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long).to(device)\n",
        "        target_ids = torch.tensor(target_ids, dtype=torch.long).to(device)\n",
        "\n",
        "        # Single forward pass (removed duplicate)\n",
        "        logits, loss = model(input_ids, targets=target_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i // BATCH_SIZE) % 50 == 0:\n",
        "            print(f\"Batch {i//BATCH_SIZE}/{len(data)//BATCH_SIZE}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Free memory\n",
        "        del logits, loss, input_ids, target_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "\n",
        "joblib.dump(model, \"v0.4.joblib\")"
      ],
      "metadata": {
        "id": "6yswq3osKlXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0518bf72-bc2d-4190-fc95-ce617391adf5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['v0.3.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer\n",
        "import joblib\n",
        "\n",
        "# Load\n",
        "model = joblib.load('v0.4.joblib') #had an issue with masking, this\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Test README\n",
        "test_readme = \"\"\"\n",
        " React UI Components\n",
        "\n",
        "A comprehensive React component library for building modern web applications with ease.\n",
        "\n",
        "Features\n",
        "\n",
        "Pre-built Components: Includes buttons, forms, modals, tooltips, and navigation components\n",
        "TypeScript Support: Fully typed components with IntelliSense support\n",
        "Dark Mode: Built-in theming system with automatic dark mode detection\n",
        "Responsive Design: Mobile-first components that adapt to any screen size\n",
        "Accessibility: WCAG 2.1 compliant with proper ARIA labels\n",
        "\n",
        "Installation\n",
        "\n",
        "\n",
        " Quick Start\n",
        "\n",
        "\n",
        "How It Works\n",
        "\n",
        "The library uses React hooks and context API for state management. Each component is built with styled-components for CSS-in-JS styling. The theming system uses CSS variables that can be toggled via a ThemeProvider wrapper. All components are tree-shakeable to minimize bundle size.\n",
        "\n",
        "Documentation\n",
        "\n",
        "Visit our docs at https://docs.example.com\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"{test_readme} SUMMARY:\"\n",
        "tokens = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    generated = model.summarize(tokens, max_new_tokens=50, tempature=1.1) #just relized i spelt it wrong\n",
        "\n",
        "result = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "print(f\"Input: {test_readme}\\n\")\n",
        "print(f\"Full output: {result}\\n\")\n",
        "print(f\"Summary only: {result.split('SUMMARY:')[-1].strip()}\")\n"
      ],
      "metadata": {
        "id": "bE-8sVkh57m8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPBJAukwMfecF20v3TcqvFP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}