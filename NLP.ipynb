{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKf9FpKDUwyr+EM8K/M2Kn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BraedynL0530/PortfolioWebsite/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWk0zbFntsd0",
        "outputId": "1c7584a0-4e08-4869-9dfa-f7d18b159e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([10, 4])\n",
            "weight: Parameter containing:\n",
            "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055],\n",
            "        [ 0.6784, -1.2345, -0.0431, -1.6047],\n",
            "        [-0.7521,  1.6487, -0.3925, -1.4036],\n",
            "        [-0.7279, -0.5594, -0.7688,  0.7624],\n",
            "        [ 1.6423, -0.1596, -0.4974,  0.4396],\n",
            "        [-0.7581,  1.0783,  0.8008,  1.6806],\n",
            "        [ 0.0349,  0.3211,  1.5736, -0.8455],\n",
            "        [ 1.3123,  0.6872, -1.0892, -0.3553],\n",
            "        [-1.4181,  0.8963,  0.0499,  2.2667],\n",
            "        [ 1.1790, -0.4345, -1.3864, -1.2862]], requires_grad=True)\n",
            "torch.Size([10, 4])\n",
            "torch.Size([8, 4])\n",
            "final emb: tensor([[[-1.5953,  0.1559,  2.6121,  1.8412],\n",
            "         [-0.3606, -0.3840,  0.6163,  0.3166],\n",
            "         [ 0.1099,  1.3950, -2.1651,  0.1804],\n",
            "         [ 2.4877,  1.2483, -1.5419, -1.1271],\n",
            "         [-0.6128,  1.3094,  0.8095,  1.5383]],\n",
            "\n",
            "        [[ 0.3418, -1.3568,  0.4250, -1.1257],\n",
            "         [ 1.0457, -1.0591,  1.3421, -2.0505],\n",
            "         [-1.9606,  1.7861, -0.2751,  2.2163],\n",
            "         [ 1.8538, -0.6734, -0.4958, -2.3764],\n",
            "         [ 1.3243, -0.2034, -1.3777, -1.4285]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import  dataclass\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#GPT-2 Example, resuse :3\n",
        "class config:\n",
        "  vocab_size: int #unique words\n",
        "  block_size: int #how far back(context) it can see, memory/ how many tokens back\n",
        "  n_layer: int = 12 # stacked blocks, more layers more reasoning more train time\n",
        "  n_head: int = 12 # attentions per layer, how many \"heads\" looking for a new pattern\n",
        "  n_embd: int = 768 #size of vector for each token\n",
        "  dropout: float = 0.1  #prevents overfitting by stopping random paths\n",
        "\n",
        "\n",
        "# Word token embedding, wte\n",
        "# nn layer\n",
        "vocab_size = 10\n",
        "n_embed = 4\n",
        "\n",
        "\n",
        "token_embedding_tabel = nn.Embedding(vocab_size, n_embed)\n",
        "\n",
        "print(\"shape:\", token_embedding_tabel.weight.shape)\n",
        "print(\"weight:\",token_embedding_tabel.weight)\n",
        "\n",
        "# Word posetional embedding, wpe\n",
        "\"\"\"\n",
        "makes a unique vector for each poestion\n",
        "vectors for each posetions, another nueral network layer\n",
        "word and poestion exist in the same n_embd dimesion space\n",
        "adding them creates a  unique point in the said space\n",
        "learns that \"the\" in the beginning postion 1 isnt the same as \"the\" at posetion 12, distictions\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "B, T, C = 2,5,n_embed #batch time/sequence length(tokens) channels(dimesions/n_embed)\n",
        "block_size = 8\n",
        "vocab_size = 10\n",
        "\n",
        "\n",
        "\n",
        "poesition_embeding_tabel = nn.Embedding(block_size, C)\n",
        "\n",
        "#input data\n",
        "idx = torch.randint(0, vocab_size, (B,T))\n",
        "\n",
        "#token emb data before\n",
        "tok_emb = token_embedding_tabel(idx)\n",
        "\n",
        "#after / poesitions for latest sequence length\n",
        "pos = torch.arange(0, T, dtype=torch.long) #just counts up from 0 and says this is index 1 2 3... for each sequnce ex T=5 returns a tensor or ([0,1,2,3,4]) helps pos tabel look uip\n",
        "pos_emb = poesition_embeding_tabel(pos)\n",
        "\n",
        "x = tok_emb + pos_emb #makes a vector with both data ex tok=[1,5,8...] pos = [-.2, .3,-4...] x =[.8, 4.7, 4]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "nn.Embeddings create and store vectors,\n",
        "torch.arrange tells what rows to return\n",
        "batches are a single sequence, could be a set token ex 128,  a Readme for a project example, paragraph senante etc MY CHOICE AS TO WHAT, T is sequence length, blocksize is max sequnce\n",
        "can go under but block_size is the limit\n",
        "\"\"\"\n",
        "\n",
        "print(token_embedding_tabel.weight.shape)\n",
        "print(poesition_embeding_tabel.weight.shape)\n",
        "print(\"final emb:\",x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import forward_ad\n",
        "\"\"\"\n",
        "self attention: part 1 of transformer\n",
        "Q K V, query key value. helps use the two embeddings to learn diffrent meanings for words and give the diffrent vectors even if the same word\n",
        "below is theory class is optimized, it condences the prjections into one huge vector and splits. other than that its nearly identical just more efficent\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "#learnable compenets\n",
        "q_prog = nn.Linear(C, C, bias =False)\n",
        "k_prog = nn.Linear(C, C, bias =False)\n",
        "v_prog = nn.Linear(C, C, bias =False)\n",
        "\n",
        "#weights\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "\n",
        "#preform projection\n",
        "q = q_prog(x)\n",
        "k = k_prog(x)\n",
        "v = v_prog(x)\n",
        "\n",
        "scores = q @ k.transpose(-2,-1)\n",
        "print(\"scores\",scores)\n",
        "\n",
        "\n",
        "\n",
        "Attention(Q,K,V)=softmax(​QK^⊤/dk​​)V\n",
        "\n",
        "d_k = k.size(-1)#last dimesion of\n",
        "scaled_scores = scores / math.sqrt(d_k)\n",
        "attention_weights = F.softmax(scaled_scores, dim=1)\n",
        "print(\"scaled scores\", scaled_scores)\n",
        "print(\"scaled scores -> percentages\", attention_weights)\n",
        "\n",
        "# aggreation Last part of attention!\n",
        "output = attention_weights @ v\n",
        "print(\"output!:\",output)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Core logic for MultiHead\n",
        "class CausualSelfAttention(nn.Module):\n",
        "  def __init__(self, *args, **kwargs) -> None:\n",
        "    super().__init__()\n",
        "    assert self.config.n_embed % config.n_head == 0\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False) # Fuzed layer = more efficent\n",
        "\n",
        "    self.register_buffer( # part of causal masking\n",
        "        \"bias\",# buffer name\n",
        "        torch.trill(torch.ones(config.block_size,config.block_size))\n",
        "        .view(1,1, config.block_size, config.block_size)\n",
        "    )\n",
        "\n",
        "    self.c_proj = nn.Linear(config.n_embd,config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()\n",
        "    head_dim = C // self.n_head\n",
        "\n",
        "    # project once -> split\n",
        "    qkv = self.c_attn(x)\n",
        "    q, k, v = qkv.split(C, dim=2)\n",
        "\n",
        "    # reshape into heads\n",
        "    q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "    # attention\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_dim))\n",
        "    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\")) # prevents it from seeing future tokens\n",
        "    att = F.softmax(att, dim=-1)\n",
        "\n",
        "    # aggregate :3\n",
        "    y = att @ v\n",
        "\n",
        "    # merge heads\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "    # final projection\n",
        "    y = self.c_proj(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "unIvygqcFwyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9MJ7SDf9p3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}