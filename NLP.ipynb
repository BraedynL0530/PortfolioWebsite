{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3bTIEjGEo4O+AKT+Yir6n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BraedynL0530/PortfolioWebsite/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# README Summary Generator - FIXED VERSION\n",
        "# Key fixes: Progress visibility, faster model, better error handling\n",
        "\n",
        "# SETUP\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q transformers torch accelerate safetensors\n",
        "\n",
        "# CONFIGURATION\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Force output flush for Colab\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Paths\n",
        "DRIVE_BASE = '/content/drive/MyDrive/readme_training'\n",
        "INPUT_FILE = f'{DRIVE_BASE}/training_data.json'\n",
        "CHECKPOINT_FILE = f'{DRIVE_BASE}/summaries_checkpoint.json'\n",
        "OUTPUT_FILE = f'{DRIVE_BASE}/summaries_final.json'\n",
        "\n",
        "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
        "\n",
        "# CRITICAL: Use a smaller, faster model for Colab\n",
        "# Pick ONE (uncomment it):\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # RECOMMENDED: Fast, no auth\n",
        "# MODEL_NAME = \"microsoft/phi-2\"  # Good quality, medium speed\n",
        "# MODEL_NAME = \"google/flan-t5-base\"  # Very fast, different architecture\n",
        "# MODEL_NAME = \"TheBloke/vicuna-7B-1.1-HF\"  # Slow but higher quality\n",
        "\n",
        "BATCH_SIZE = 1  # Process one at a time for better progress tracking\n",
        "CHECKPOINT_INTERVAL = 5  # Save every 5 summaries\n",
        "\n",
        "print(f\"‚úÖ Model: {MODEL_NAME}\")\n",
        "print(f\"‚úÖ Checkpoint every: {CHECKPOINT_INTERVAL}\")\n",
        "print(f\"‚úÖ Drive path: {DRIVE_BASE}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# LOAD MODEL\n",
        "print(\"\\nüì¶ Loading model... (this may take 2-5 minutes)\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "    print(f\"‚úÖ Using device: {model.device}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå MODEL LOAD FAILED: {e}\")\n",
        "    print(\"Try using: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    sys.stdout.flush()\n",
        "    raise\n",
        "\n",
        "# LOAD DATA\n",
        "print(\"\\nüìÇ Loading README data...\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "with open(INPUT_FILE, 'r') as f:\n",
        "    readmes_data = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(readmes_data)} READMEs\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Load checkpoint\n",
        "try:\n",
        "    with open(CHECKPOINT_FILE, 'r') as f:\n",
        "        processed_summaries = json.load(f)\n",
        "    processed_indices = {s['id'] for s in processed_summaries}\n",
        "    print(f\"üìã Resuming: {len(processed_summaries)} already done\")\n",
        "except FileNotFoundError:\n",
        "    processed_summaries = []\n",
        "    processed_indices = set()\n",
        "    print(\"üìã Starting fresh\")\n",
        "\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Add unique IDs if missing\n",
        "for i, item in enumerate(readmes_data):\n",
        "    if 'id' not in item:\n",
        "        item['id'] = i\n",
        "\n",
        "remaining = [r for r in readmes_data if r['id'] not in processed_indices]\n",
        "print(f\"üìä Remaining: {len(remaining)}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# SUMMARY GENERATOR\n",
        "def generate_summary(readme_text, max_length=1500):\n",
        "    \"\"\"Generate summary with better error handling\"\"\"\n",
        "\n",
        "    # Truncate\n",
        "    if len(readme_text) > max_length:\n",
        "        readme_text = readme_text[:max_length] + \"...\"\n",
        "\n",
        "    # Improved prompt for better technical summaries\n",
        "    prompt = f\"\"\"Write a technical summary of this GitHub project as a single paragraph with 3-4 complete sentences.\n",
        "\n",
        "Your summary should cover:\n",
        "- What the project does and its main purpose\n",
        "- Technologies used (programming languages, frameworks, libraries) - only mention what's in the README\n",
        "- Key features or implementation details\n",
        "\n",
        "Do NOT use numbered lists or bullet points. Write in natural paragraph form.\n",
        "\n",
        "README:\n",
        "{readme_text}\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,  # Reduced for speed\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract summary - now looking for \"Technical Summary:\"\n",
        "        if \"Technical Summary:\" in full_output:\n",
        "            summary = full_output.split(\"Technical Summary:\")[-1].strip()\n",
        "        elif \"Summary:\" in full_output:\n",
        "            summary = full_output.split(\"Summary:\")[-1].strip()\n",
        "        else:\n",
        "            summary = full_output[len(prompt):].strip()\n",
        "\n",
        "        # Clean up\n",
        "        summary = summary.replace('\\n', ' ').strip()\n",
        "\n",
        "        # Remove any trailing incomplete sentences (but keep full summary)\n",
        "        # Only trim if it's unreasonably long (over 1000 chars)\n",
        "        if len(summary) > 1000:\n",
        "            # Try to cut at last sentence\n",
        "            last_period = summary[:1000].rfind('.')\n",
        "            if last_period > 500:\n",
        "                summary = summary[:last_period + 1]\n",
        "\n",
        "        return summary\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Generation error: {e}\")\n",
        "        return f\"Error generating summary: {str(e)[:100]}\"\n",
        "\n",
        "# MAIN PROCESSING LOOP\n",
        "print(\"\\nüöÄ Starting generation...\\n\")\n",
        "print(\"=\" * 60)\n",
        "sys.stdout.flush()\n",
        "\n",
        "for i, readme_data in enumerate(remaining):\n",
        "    try:\n",
        "        print(f\"\\n[{i+1}/{len(remaining)}] Processing: {readme_data.get('repo_name', 'Unknown')}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Generate\n",
        "        summary = generate_summary(readme_data['readme'])\n",
        "\n",
        "        # Save result\n",
        "        readme_data['summary'] = summary\n",
        "        processed_summaries.append(readme_data)\n",
        "\n",
        "        # Show FULL summary for first 5, then preview for rest\n",
        "        if i < 5:\n",
        "            print(f\"   ‚úì FULL: {summary}\")\n",
        "        else:\n",
        "            preview = summary[:100] + \"...\" if len(summary) > 100 else summary\n",
        "            print(f\"   ‚úì {preview}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Checkpoint\n",
        "        if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
        "            with open(CHECKPOINT_FILE, 'w') as f:\n",
        "                json.dump(processed_summaries, f, indent=2)\n",
        "            print(f\"\\nüíæ CHECKPOINT SAVED: {len(processed_summaries)} summaries\")\n",
        "            print(\"=\" * 60)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå FAILED {readme_data.get('repo_name', 'Unknown')}: {e}\")\n",
        "        sys.stdout.flush()\n",
        "        continue\n",
        "\n",
        "# FINAL SAVE\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üíæ Saving final results...\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "with open(CHECKPOINT_FILE, 'w') as f:\n",
        "    json.dump(processed_summaries, f, indent=2)\n",
        "\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    json.dump(processed_summaries, f, indent=2)\n",
        "\n",
        "print(f\"\"\"\n",
        "‚ú® COMPLETE! ‚ú®\n",
        "\n",
        "üìä Stats:\n",
        "   Total: {len(processed_summaries)} summaries\n",
        "   Checkpoint: {CHECKPOINT_FILE}\n",
        "   Final: {OUTPUT_FILE}\n",
        "\n",
        "üéØ Next: Download from Google Drive and train your model!\n",
        "\"\"\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# PREVIEW\n",
        "print(\"\\nüìã Sample summaries:\")\n",
        "for i, item in enumerate(processed_summaries[:3]):\n",
        "    print(f\"\\n{i+1}. {item.get('repo_name', 'Unknown')} ({item.get('stars', 0)} ‚≠ê)\")\n",
        "    print(f\"   {item['summary']}\")\n",
        "sys.stdout.flush()"
      ],
      "metadata": {
        "id": "nQ7xNvJ17NHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "MWk0zbFntsd0",
        "outputId": "97cabf66-ff10-4a6c-8cdf-97c78a5937da"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'n_embd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-729389705.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtoken_embedding_tabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_embedding_tabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_embd' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import  dataclass\n",
        "torch.manual_seed(42)\n",
        "\"\"\"#GPT-2 Example, resuse :3\n",
        "class config:\n",
        "  vocab_size: int #unique words\n",
        "  block_size: int #how far back(context) it can see, memory/ how many tokens back\n",
        "  n_layer: int = 12 # stacked blocks, more layers more reasoning more train time\n",
        "  n_head: int = 12 # attentions per layer, how many \"heads\" looking for a new pattern\n",
        "  n_embd: int = 768 #size of vector for each token\n",
        "  dropout: float = 0.1  #prevents overfitting by stopping random paths\"\"\"\n",
        "\n",
        "\n",
        "# Word token embedding, wte\n",
        "# nn layer\n",
        "vocab_size = 10\n",
        "n_embed = 4\n",
        "\n",
        "\n",
        "token_embedding_tabel = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "print(\"shape:\", token_embedding_tabel.weight.shape)\n",
        "print(\"weight:\",token_embedding_tabel.weight)\n",
        "\n",
        "# Word posetional embedding, wpe\n",
        "\"\"\"\n",
        "makes a unique vector for each poestion\n",
        "vectors for each posetions, another nueral network layer\n",
        "word and poestion exist in the same n_embd dimesion space\n",
        "adding them creates a  unique point in the said space\n",
        "learns that \"the\" in the beginning postion 1 isnt the same as \"the\" at posetion 12, distictions\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "B, T, C = 2,5,n_embed #batch time/sequence length(tokens) channels(dimesions/n_embed)\n",
        "block_size = 8\n",
        "vocab_size = 10\n",
        "\n",
        "\n",
        "\n",
        "poesition_embeding_tabel = nn.Embedding(block_size, C)\n",
        "\n",
        "#input data\n",
        "idx = torch.randint(0, vocab_size, (B,T))\n",
        "\n",
        "#token emb data before\n",
        "tok_emb = token_embedding_tabel(idx)\n",
        "\n",
        "#after / poesitions for latest sequence length\n",
        "pos = torch.arange(0, T, dtype=torch.long) #just counts up from 0 and says this is index 1 2 3... for each sequnce ex T=5 returns a tensor or ([0,1,2,3,4]) helps pos tabel look uip\n",
        "pos_emb = poesition_embeding_tabel(pos)\n",
        "\n",
        "x = tok_emb + pos_emb #makes a vector with both data ex tok=[1,5,8...] pos = [-.2, .3,-4...] x =[.8, 4.7, 4]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "nn.Embeddings create and store vectors,\n",
        "torch.arrange tells what rows to return\n",
        "batches are a single sequence, could be a set token ex 128,  a Readme for a project example, paragraph senante etc MY CHOICE AS TO WHAT, T is sequence length, blocksize is max sequnce\n",
        "can go under but block_size is the limit\n",
        "\"\"\"\n",
        "\n",
        "print(token_embedding_tabel.weight.shape)\n",
        "print(poesition_embeding_tabel.weight.shape)\n",
        "print(\"final emb:\",x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import forward_ad\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import  dataclass\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#config\n",
        "@dataclass\n",
        "class config:\n",
        "  vocab_size: int #unique words\n",
        "  block_size: int #how far back(context) it can see, memory/ how many tokens back\n",
        "  n_layer: int = 6 # stacked blocks, more layers more reasoning more train time\n",
        "  n_head: int = 12 # attentions per layer, how many \"eyes\" looking for a new pattern\n",
        "  n_embd: int = 768 #size of vector for each token\n",
        "  dropout: float = 0.2  #prevents overfitting by stopping random paths\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "self attention: part 1 of transformer\n",
        "Q K V, query key value. helps use the two embeddings to learn diffrent meanings for words and give the diffrent vectors even if the same word\n",
        "below is theory class is optimized, it condences the prjections into one huge vector and splits. other than that its nearly identical just more efficent\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "#learnable compenets\n",
        "q_prog = nn.Linear(C, C, bias =False)\n",
        "k_prog = nn.Linear(C, C, bias =False)\n",
        "v_prog = nn.Linear(C, C, bias =False)\n",
        "\n",
        "#weights\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "\n",
        "#preform projection\n",
        "q = q_prog(x)\n",
        "k = k_prog(x)\n",
        "v = v_prog(x)\n",
        "\n",
        "scores = q @ k.transpose(-2,-1)\n",
        "print(\"scores\",scores)\n",
        "\n",
        "\n",
        "\n",
        "Attention(Q,K,V)=softmax(‚ÄãQK^‚ä§/dk‚Äã‚Äã)V\n",
        "\n",
        "d_k = k.size(-1)#last dimesion of\n",
        "scaled_scores = scores / math.sqrt(d_k)\n",
        "attention_weights = F.softmax(scaled_scores, dim=1)\n",
        "print(\"scaled scores\", scaled_scores)\n",
        "print(\"scaled scores -> percentages\", attention_weights)\n",
        "\n",
        "# aggreation Last part of attention!\n",
        "output = attention_weights @ v\n",
        "print(\"output!:\",output)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Core logic for MultiHead\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config :config):\n",
        "    super().__init__()\n",
        "    assert self.config.n_embed % config.n_head == 0\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False) # Fuzed layer = more efficent\n",
        "\n",
        "    self.register_buffer( # part of causal masking\n",
        "        \"bias\",# buffer name\n",
        "        torch.tril(torch.ones(config.block_size,config.block_size))\n",
        "        .view(1,1, config.block_size, config.block_size)\n",
        "    )\n",
        "\n",
        "    self.c_proj = nn.Linear(config.n_embd,config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()\n",
        "    head_dim = C // self.n_head\n",
        "\n",
        "    # project once -> split\n",
        "    qkv = self.c_attn(x)\n",
        "    q, k, v = qkv.split(C, dim=2)\n",
        "\n",
        "    # reshape into heads\n",
        "    q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "    # attention\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_dim))\n",
        "    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\")) # prevents it from seeing future tokens\n",
        "    att = F.softmax(att, dim=-1)\n",
        "\n",
        "    # aggregate :3\n",
        "    y = att @ v\n",
        "\n",
        "    # merge heads\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "    # final projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config :config):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(config.n_embd, 4 * config.n_embd) #expands dimestions, think of it as more room to think / combining features\n",
        "    self.proj = nn.Linear(4 * config.n_embd, config.n_embd) # condenses back so it can be added back to attetion\n",
        "    self.drop = nn.Dropout(config.dropout) #refer to config\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = F.gelu(x) # makes x nonlinear so fc and proj dont just merge into one straight line\n",
        "    x =self.proj(x)\n",
        "    x = self.drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module): #residual connection\n",
        "  def __init__(self, config : config): #litterly just does f(x) + x instead of f(x) so mlp dosesnt relearn it takes the learned/trained data and keeps it\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # focus (the \"+\")\n",
        "    x = x +self.attn(self.ln_1(x))\n",
        "\n",
        "    x = x +self.mlp(self.ln_2(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "unIvygqcFwyc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLP(nn.Module):\n",
        "  def __init__(self, config: config):\n",
        "    super().__init__()\n",
        "    # Input\n",
        "    self.wte = nn.Embedding(vocab_size, config.n_embd)\n",
        "    tok_emb = self.wte(idx)\n",
        "    self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
        "    pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "    pos_emb = self.wpe(pos)\n",
        "    self.drop = nn.Dropout(config.dropout)\n",
        "\n",
        "    x = self.drop(tok_emb + pos_emb)\n",
        "\n",
        "    # Processing, makes a stack/block / LAYER for deeper understanding\n",
        "    # Data flows through sequncesnsy so more refined/better understanding\n",
        "    self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "\n",
        "    #output layers\n",
        "    self.ln_f = nn.LayerNorm(config.n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False) #language model head, parrel prediction(linear) makes raw score for each possible next token , good for training, and throws away the\n",
        "    #rest(all but last vector) if not traning\n",
        "    # Above makes raw score for each possible next token\n",
        "\n",
        "\n",
        "    self.lm_head.weight = self.wte.weight\n",
        "\n",
        "  def forward(self, idx, targets= None): #idx = input targets = inputs shifter one pos to left\n",
        "    B, T = idx.size()\n",
        "    #embdedings dropounts blocks layernorm... makes logits tensors\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=tokenizer.pad_token_id) # makes it 2d for loss calc for each B*T prediction and takes mean of all of them to make loss\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad\n",
        "  def summarize(self, idx, max_new_tokens, tempature = 1.0, top_k =0): # <1 rare words more likely, >1 more common words\n",
        "    # Crop context if too long\n",
        "    idx_cond = idx[: -self.config.block_size:]\n",
        "\n",
        "    # Predict/get  lodgits\n",
        "    logits, _ = self(idx_cond)\n",
        "    logits = logits[: -1, :] / tempature\n",
        "\n",
        "\n",
        "    # convert to probs and sample\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    next_token = torch.multimonial(probs, num_samples=1)\n",
        "\n",
        "    # add new token to sequence\n",
        "    idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "d9MJ7SDf9p3Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HHZ3z6SdsnYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "with open(OUTPUT_FILE) as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    for i in range(0, len(data), BATCH_SIZE):\n",
        "\n",
        "        # Get the batch slice\n",
        "        batch = data[i:i+BATCH_SIZE]\n",
        "\n",
        "        # Tokenize all examples in batch\n",
        "        batch_tokens = []\n",
        "        for item in batch:\n",
        "            full_text = f\"{item['readme']} SUMMARY: {item['summary']}\"\n",
        "            tokens = tokenizer.encode(full_text, max_length=512, truncation=True)\n",
        "\n",
        "            # Pad to same length\n",
        "            if len(tokens) < 512:\n",
        "                tokens = tokens + [tokenizer.pad_token_id] * (512 - len(tokens))\n",
        "\n",
        "            batch_tokens.append(tokens[:512])\n",
        "\n",
        "        # Convert to tensor\n",
        "        batch_tokens = torch.tensor(batch_tokens, dtype=torch.long).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        model = NLP(config_instance).to(device)\n",
        "        logits, loss = model(batch_tokens, targets=batch_tokens)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i // BATCH_SIZE) % 50 == 0:\n",
        "            print(f\"Batch {i//BATCH_SIZE}/{len(data)//BATCH_SIZE}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "7-PUEltGnIF5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}