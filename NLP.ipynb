{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxhu/JbWBJijUVTuq9dL4f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BraedynL0530/PortfolioWebsite/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# README Summary Generator for Training Data\n",
        "# Checkpoints so collab dosent cut off traing data generation(traing later once model is done)\n",
        "\n",
        "\n",
        "# SETUP\n",
        "\n",
        "# Mount Google Drive for checkpointing\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q transformers torch accelerate\n",
        "\n",
        "# CONFIGURATION\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Paths\n",
        "DRIVE_BASE = '/content/drive/MyDrive/readme_training'\n",
        "INPUT_FILE = f'{DRIVE_BASE}/training_data.json'  # From Go scraper\n",
        "CHECKPOINT_FILE = f'{DRIVE_BASE}/summaries_checkpoint.json'\n",
        "OUTPUT_FILE = f'{DRIVE_BASE}/summaries_final.json'\n",
        "\n",
        "# Create directory if needed\n",
        "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
        "\n",
        "# Model config\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "BATCH_SIZE = 5  # Process N at a time before saving\n",
        "CHECKPOINT_INTERVAL = 10  # Save every N summaries\n",
        "\n",
        "print(f\"‚úÖ Using model: {MODEL_NAME}\")\n",
        "print(f\"‚úÖ Checkpointing every {CHECKPOINT_INTERVAL} summaries\")\n",
        "print(f\"‚úÖ Drive path: {DRIVE_BASE}\")\n",
        "\n",
        "# LOAD MODEL\n",
        "\n",
        "print(\"\\nüì¶ Loading model...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,  # Faster on GPU\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# Set pad token if needed\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"‚úÖ Model loaded!\")\n",
        "\n",
        "\n",
        "# LOAD DATA\n",
        "\n",
        "\n",
        "print(\"\\nüìÇ Loading README data...\")\n",
        "\n",
        "# Load scraped READMEs\n",
        "with open(INPUT_FILE, 'r') as f:\n",
        "    readmes_data = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(readmes_data)} READMEs\")\n",
        "\n",
        "# Load existing checkpoint if available\n",
        "try:\n",
        "    with open(CHECKPOINT_FILE, 'r') as f:\n",
        "        processed_summaries = json.load(f)\n",
        "    processed_names = {s['repo_name'] for s in processed_summaries}\n",
        "    print(f\"üìã Resuming from checkpoint: {len(processed_summaries)} already done\")\n",
        "except FileNotFoundError:\n",
        "    processed_summaries = []\n",
        "    processed_names = set()\n",
        "    print(\"üìã Starting fresh (no checkpoint found)\")\n",
        "\n",
        "# Filter out already processed\n",
        "remaining = [r for r in readmes_data if r['repo_name'] not in processed_names]\n",
        "print(f\"üìä Remaining to process: {len(remaining)}\")\n",
        "\n",
        "# ============================================================\n",
        "# SUMMARY GENERATOR\n",
        "# ============================================================\n",
        "\n",
        "def generate_summary(readme_text, repo_name=\"\", max_readme_length=2000):\n",
        "    \"\"\"Generate a 2-3 sentence summary of a README\"\"\"\n",
        "\n",
        "    # Truncate if too long (Llama has token limits)\n",
        "    if len(readme_text) > max_readme_length:\n",
        "        readme_text = readme_text[:max_readme_length] + \"...\"\n",
        "\n",
        "    prompt = f\"\"\"Summarize this GitHub README in exactly 2-3 sentences. Focus on what the project does and its key features.\n",
        "\n",
        "README:\n",
        "{readme_text}\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=2048\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and extract just the summary part\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract summary (everything after \"Summary:\")\n",
        "    if \"Summary:\" in full_output:\n",
        "        summary = full_output.split(\"Summary:\")[-1].strip()\n",
        "    else:\n",
        "        summary = full_output[len(prompt):].strip()\n",
        "\n",
        "    # Clean up\n",
        "    summary = summary.replace('\\n', ' ').strip()\n",
        "\n",
        "    return summary\n",
        "\n",
        "# PROCESS IN BATCHES WITH CHECKPOINTING\n",
        "\n",
        "print(\"\\nüöÄ Starting summary generation...\\n\")\n",
        "\n",
        "for i in tqdm(range(0, len(remaining), BATCH_SIZE)):\n",
        "    batch = remaining[i:i+BATCH_SIZE]\n",
        "\n",
        "    for readme_data in batch:\n",
        "        try:\n",
        "            summary = generate_summary(\n",
        "                readme_data['readme'],\n",
        "                readme_data['repo_name']\n",
        "            )\n",
        "\n",
        "            # Add summary to data\n",
        "            readme_data['summary'] = summary\n",
        "            processed_summaries.append(readme_data)\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"\\n‚úÖ {readme_data['owner']}/{readme_data['repo_name']}\")\n",
        "            print(f\"   üìù {summary[:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Failed {readme_data['repo_name']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Save checkpoint after each batch\n",
        "    if len(processed_summaries) % CHECKPOINT_INTERVAL == 0:\n",
        "        with open(CHECKPOINT_FILE, 'w') as f:\n",
        "            json.dump(processed_summaries, f, indent=2)\n",
        "        print(f\"\\nüíæ Checkpoint saved: {len(processed_summaries)} summaries\")\n",
        "\n",
        "#\n",
        "# FINAL SAVE\n",
        "#\n",
        "\n",
        "print(\"\\nüíæ Saving final results...\")\n",
        "\n",
        "# Save checkpoint (in case of issues)\n",
        "with open(CHECKPOINT_FILE, 'w') as f:\n",
        "    json.dump(processed_summaries, f, indent=2)\n",
        "\n",
        "# Save final output\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    json.dump(processed_summaries, f, indent=2)\n",
        "\n",
        "print(f\"\"\"\n",
        "‚ú® COMPLETE! ‚ú®\n",
        "\n",
        "üìä Stats:\n",
        "   - Total summaries: {len(processed_summaries)}\n",
        "   - Checkpoint: {CHECKPOINT_FILE}\n",
        "   - Final output: {OUTPUT_FILE}\n",
        "\n",
        "üéØ Next steps:\n",
        "   1. Download {OUTPUT_FILE} from Google Drive\n",
        "   2. Use it to train your PyTorch model\n",
        "   3. Ship your portfolio!\n",
        "\"\"\")\n",
        "\n",
        "#\n",
        "# PREVIEW RESULTS\n",
        "#\n",
        "\n",
        "print(\"\\nüìã Sample summaries:\")\n",
        "for i, item in enumerate(processed_summaries[:3]):\n",
        "    print(f\"\\n{i+1}. {item['owner']}/{item['repo_name']} ({item['stars']} ‚≠ê)\")\n",
        "    print(f\"   {item['summary']}\")"
      ],
      "metadata": {
        "id": "nQ7xNvJ17NHf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "10d73fff-5ff3-4f2b-f82e-a1ebd999e34f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3377552933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Mount Google Drive for checkpointing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Install dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWk0zbFntsd0",
        "outputId": "1c7584a0-4e08-4869-9dfa-f7d18b159e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([10, 4])\n",
            "weight: Parameter containing:\n",
            "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055],\n",
            "        [ 0.6784, -1.2345, -0.0431, -1.6047],\n",
            "        [-0.7521,  1.6487, -0.3925, -1.4036],\n",
            "        [-0.7279, -0.5594, -0.7688,  0.7624],\n",
            "        [ 1.6423, -0.1596, -0.4974,  0.4396],\n",
            "        [-0.7581,  1.0783,  0.8008,  1.6806],\n",
            "        [ 0.0349,  0.3211,  1.5736, -0.8455],\n",
            "        [ 1.3123,  0.6872, -1.0892, -0.3553],\n",
            "        [-1.4181,  0.8963,  0.0499,  2.2667],\n",
            "        [ 1.1790, -0.4345, -1.3864, -1.2862]], requires_grad=True)\n",
            "torch.Size([10, 4])\n",
            "torch.Size([8, 4])\n",
            "final emb: tensor([[[-1.5953,  0.1559,  2.6121,  1.8412],\n",
            "         [-0.3606, -0.3840,  0.6163,  0.3166],\n",
            "         [ 0.1099,  1.3950, -2.1651,  0.1804],\n",
            "         [ 2.4877,  1.2483, -1.5419, -1.1271],\n",
            "         [-0.6128,  1.3094,  0.8095,  1.5383]],\n",
            "\n",
            "        [[ 0.3418, -1.3568,  0.4250, -1.1257],\n",
            "         [ 1.0457, -1.0591,  1.3421, -2.0505],\n",
            "         [-1.9606,  1.7861, -0.2751,  2.2163],\n",
            "         [ 1.8538, -0.6734, -0.4958, -2.3764],\n",
            "         [ 1.3243, -0.2034, -1.3777, -1.4285]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import  dataclass\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#GPT-2 Example, resuse :3\n",
        "class config:\n",
        "  vocab_size: int #unique words\n",
        "  block_size: int #how far back(context) it can see, memory/ how many tokens back\n",
        "  n_layer: int = 12 # stacked blocks, more layers more reasoning more train time\n",
        "  n_head: int = 12 # attentions per layer, how many \"heads\" looking for a new pattern\n",
        "  n_embd: int = 768 #size of vector for each token\n",
        "  dropout: float = 0.1  #prevents overfitting by stopping random paths\"\"\"\n",
        "\n",
        "\n",
        "# Word token embedding, wte\n",
        "# nn layer\n",
        "vocab_size = 10\n",
        "n_embed = 4\n",
        "\n",
        "\n",
        "token_embedding_tabel = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "print(\"shape:\", token_embedding_tabel.weight.shape)\n",
        "print(\"weight:\",token_embedding_tabel.weight)\n",
        "\n",
        "# Word posetional embedding, wpe\n",
        "\"\"\"\n",
        "makes a unique vector for each poestion\n",
        "vectors for each posetions, another nueral network layer\n",
        "word and poestion exist in the same n_embd dimesion space\n",
        "adding them creates a  unique point in the said space\n",
        "learns that \"the\" in the beginning postion 1 isnt the same as \"the\" at posetion 12, distictions\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "B, T, C = 2,5,n_embed #batch time/sequence length(tokens) channels(dimesions/n_embed)\n",
        "block_size = 8\n",
        "vocab_size = 10\n",
        "\n",
        "\n",
        "\n",
        "poesition_embeding_tabel = nn.Embedding(block_size, C)\n",
        "\n",
        "#input data\n",
        "idx = torch.randint(0, vocab_size, (B,T))\n",
        "\n",
        "#token emb data before\n",
        "tok_emb = token_embedding_tabel(idx)\n",
        "\n",
        "#after / poesitions for latest sequence length\n",
        "pos = torch.arange(0, T, dtype=torch.long) #just counts up from 0 and says this is index 1 2 3... for each sequnce ex T=5 returns a tensor or ([0,1,2,3,4]) helps pos tabel look uip\n",
        "pos_emb = poesition_embeding_tabel(pos)\n",
        "\n",
        "x = tok_emb + pos_emb #makes a vector with both data ex tok=[1,5,8...] pos = [-.2, .3,-4...] x =[.8, 4.7, 4]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "nn.Embeddings create and store vectors,\n",
        "torch.arrange tells what rows to return\n",
        "batches are a single sequence, could be a set token ex 128,  a Readme for a project example, paragraph senante etc MY CHOICE AS TO WHAT, T is sequence length, blocksize is max sequnce\n",
        "can go under but block_size is the limit\n",
        "\"\"\"\n",
        "\n",
        "print(token_embedding_tabel.weight.shape)\n",
        "print(poesition_embeding_tabel.weight.shape)\n",
        "print(\"final emb:\",x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import forward_ad\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import  dataclass\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#config\n",
        "class config:\n",
        "  vocab_size: int #unique words\n",
        "  block_size: int #how far back(context) it can see, memory/ how many tokens back\n",
        "  n_layer: int = 12 # stacked blocks, more layers more reasoning more train time\n",
        "  n_head: int = 12 # attentions per layer, how many \"heads\" looking for a new pattern\n",
        "  n_embd: int = 768 #size of vector for each token\n",
        "  dropout: float = 0.1  #prevents overfitting by stopping random paths\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "self attention: part 1 of transformer\n",
        "Q K V, query key value. helps use the two embeddings to learn diffrent meanings for words and give the diffrent vectors even if the same word\n",
        "below is theory class is optimized, it condences the prjections into one huge vector and splits. other than that its nearly identical just more efficent\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "#learnable compenets\n",
        "q_prog = nn.Linear(C, C, bias =False)\n",
        "k_prog = nn.Linear(C, C, bias =False)\n",
        "v_prog = nn.Linear(C, C, bias =False)\n",
        "\n",
        "#weights\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "\n",
        "#preform projection\n",
        "q = q_prog(x)\n",
        "k = k_prog(x)\n",
        "v = v_prog(x)\n",
        "\n",
        "scores = q @ k.transpose(-2,-1)\n",
        "print(\"scores\",scores)\n",
        "\n",
        "\n",
        "\n",
        "Attention(Q,K,V)=softmax(‚ÄãQK^‚ä§/dk‚Äã‚Äã)V\n",
        "\n",
        "d_k = k.size(-1)#last dimesion of\n",
        "scaled_scores = scores / math.sqrt(d_k)\n",
        "attention_weights = F.softmax(scaled_scores, dim=1)\n",
        "print(\"scaled scores\", scaled_scores)\n",
        "print(\"scaled scores -> percentages\", attention_weights)\n",
        "\n",
        "# aggreation Last part of attention!\n",
        "output = attention_weights @ v\n",
        "print(\"output!:\",output)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Core logic for MultiHead\n",
        "class CausualSelfAttention(nn.Module):\n",
        "  def __init__(self, config :config)\n",
        "    super().__init__()\n",
        "    assert self.config.n_embed % config.n_head == 0\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False) # Fuzed layer = more efficent\n",
        "\n",
        "    self.register_buffer( # part of causal masking\n",
        "        \"bias\",# buffer name\n",
        "        torch.trill(torch.ones(config.block_size,config.block_size))\n",
        "        .view(1,1, config.block_size, config.block_size)\n",
        "    )\n",
        "\n",
        "    self.c_proj = nn.Linear(config.n_embd,config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()\n",
        "    head_dim = C // self.n_head\n",
        "\n",
        "    # project once -> split\n",
        "    qkv = self.c_attn(x)\n",
        "    q, k, v = qkv.split(C, dim=2)\n",
        "\n",
        "    # reshape into heads\n",
        "    q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "    # attention\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_dim))\n",
        "    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\")) # prevents it from seeing future tokens\n",
        "    att = F.softmax(att, dim=-1)\n",
        "\n",
        "    # aggregate :3\n",
        "    y = att @ v\n",
        "\n",
        "    # merge heads\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "    # final projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config :config):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(config.n_embd, 4 * config.n_embd) #expands dimestions, think of it as more room to think / combining features\n",
        "    self.proj = nn.Linear(4 * config.n_embd, config.n_embd) # condenses back so it can be added back to attetion\n",
        "    self.drop = nn.Dropout(config.dropout) #refer to config\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = F.gelu(x) # makes x nonlinear so fc and proj dont just merge into one straight line\n",
        "    x =self.proj(x)\n",
        "    x = self.drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module): #residual connection\n",
        "  def __init__(self, config : config): #litterly just does f(x) + x instead of f(x) so mlp dosesnt relearn it takes the learned/trained data and keeps it\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CasualSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # focus (the \"+\")\n",
        "    x = x +self.attn(self.ln_1(x))\n",
        "\n",
        "    x = x +self.mlp(self.ln_2(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "unIvygqcFwyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLP(nn.Module):\n",
        "  def __init__(self, config: config):\n",
        "    super().__init__()\n",
        "    # Input\n",
        "    self.wte = nn.Embedding(vocab_size, config.n_embd)\n",
        "    self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
        "    self.drop = nn.Dropout(config.dropout)\n",
        "\n",
        "    # Processing, makes a stack/block / LAYER for deeper understanding\n",
        "    # Data flows through sequncesnsy so more refined/better understanding\n",
        "    self.h = nn.ModuleList([Block(config) for _ in range(config.n_layers)])\n",
        "\n",
        "    #output layers\n",
        "    self.ln_f = nn.LayerNorm(config.n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False) #language model head, parrel prediction(linear) makes raw score for each possible next token , good for training, and throws away the\n",
        "    #rest(all but last vector) if not traning\n",
        "    # Above makes raw score for each possible next token\n",
        "\n",
        "\n",
        "    self.lm_head.weight = self.wte.weight\n",
        "\n",
        "  def forward(self, idx, targets= None): #idx = input targets = inputs shifter one pos to left\n",
        "    B, T = idx.size()\n",
        "    #embdedings dropounts blocks layernorm... makes logits tensors\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      loss = F.Cross_entrophy(logits.view(-1, logits.size(-1)), targets.view(-1)) # makes it 2d for loss calc for each B*T prediction and takes mean of all of them to make loss\n",
        "\n",
        "    return logits, loss"
      ],
      "metadata": {
        "id": "d9MJ7SDf9p3Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "a1a31424-c5ab-439d-b077-a3a08b31dac7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1506255164.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "psudeo code/ placeholders\n",
        "readme_ids = tokenized(for readme or whatever in data)\n",
        "summarry_ids - tokenizer(same as above)\n",
        "targets_shifted = summary_ids[:, 1:]\n",
        "logits, loss = NLP(idx = readme_ids, targets = targets_shifted)"
      ],
      "metadata": {
        "id": "t8fh-ECN847W"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}