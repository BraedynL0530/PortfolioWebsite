{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BraedynL0530/PortfolioWebsite/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ7xNvJ17NHf"
      },
      "outputs": [],
      "source": [
        "# README Summary Generator - FIXED VERSION\n",
        "# Key fixes: Progress visibility, faster model, better error handling\n",
        "\n",
        "# SETUP\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q transformers torch accelerate safetensors\n",
        "\n",
        "# CONFIGURATION\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Force output flush for Colab\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Paths\n",
        "DRIVE_BASE = '/content/drive/MyDrive/readme_training'\n",
        "INPUT_FILE = f'{DRIVE_BASE}/training_data.json'\n",
        "CHECKPOINT_FILE = f'{DRIVE_BASE}/summaries_checkpoint.json'\n",
        "OUTPUT_FILE = f'{DRIVE_BASE}/summaries_final.json'\n",
        "\n",
        "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
        "\n",
        "# CRITICAL: Use a smaller, faster model for Colab\n",
        "# Pick ONE (uncomment it):\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "BATCH_SIZE = 1  # Process one at a time for better progress tracking\n",
        "CHECKPOINT_INTERVAL = 5  # Save every 5 summaries\n",
        "\n",
        "print(f\"‚úÖ Model: {MODEL_NAME}\")\n",
        "print(f\"‚úÖ Checkpoint every: {CHECKPOINT_INTERVAL}\")\n",
        "print(f\"‚úÖ Drive path: {DRIVE_BASE}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# LOAD MODEL\n",
        "print(\"\\nüì¶ Loading model... (this may take 2-5 minutes)\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "    print(f\"‚úÖ Using device: {model.device}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå MODEL LOAD FAILED: {e}\")\n",
        "    print(\"Try using: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    sys.stdout.flush()\n",
        "    raise\n",
        "\n",
        "# LOAD DATA\n",
        "print(\"\\nüìÇ Loading README data...\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "with open(INPUT_FILE, 'r') as f:\n",
        "    readmes_data = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(readmes_data)} READMEs\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Load checkpoint\n",
        "try:\n",
        "    with open(CHECKPOINT_FILE, 'r') as f:\n",
        "        processed_summaries = json.load(f)\n",
        "    processed_indices = {s['id'] for s in processed_summaries}\n",
        "    print(f\"üìã Resuming: {len(processed_summaries)} already done\")\n",
        "except FileNotFoundError:\n",
        "    processed_summaries = []\n",
        "    processed_indices = set()\n",
        "    print(\"üìã Starting fresh\")\n",
        "\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Add unique IDs if missing\n",
        "for i, item in enumerate(readmes_data):\n",
        "    if 'id' not in item:\n",
        "        item['id'] = i\n",
        "\n",
        "remaining = [r for r in readmes_data if r['id'] not in processed_indices]\n",
        "print(f\"üìä Remaining: {len(remaining)}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# SUMMARY GENERATOR\n",
        "def generate_summary(readme_text, max_length=1500):\n",
        "    \"\"\"Generate summary with better error handling\"\"\"\n",
        "\n",
        "    # Truncate\n",
        "    if len(readme_text) > max_length:\n",
        "        readme_text = readme_text[:max_length] + \"...\"\n",
        "\n",
        "    # Improved prompt for better technical summaries\n",
        "    prompt = f\"\"\"Summarize this README in 4 lines:\n",
        "1. Purpose:\n",
        "2. Languages:\n",
        "3. Frameworks:\n",
        "4. Features:\n",
        "\n",
        "If information is missing write \"not specified\". Do not make up information.\n",
        "\n",
        "{readme_text}\n",
        "\n",
        "Summary:\n",
        "1. Purpose:\"\"\"\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,  # Reduced for speed\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract summary - now looking for \"Technical Summary:\"\n",
        "        if \"Technical Summary:\" in full_output:\n",
        "            summary = full_output.split(\"Technical Summary:\")[-1].strip()\n",
        "        elif \"Summary:\" in full_output:\n",
        "            summary = full_output.split(\"Summary:\")[-1].strip()\n",
        "        else:\n",
        "            summary = full_output[len(prompt):].strip()\n",
        "\n",
        "        # Clean up\n",
        "        summary = summary.replace('\\n', ' ').strip()\n",
        "\n",
        "        # Remove any trailing incomplete sentences (but keep full summary)\n",
        "        # Only trim if it's unreasonably long (over 1000 chars)\n",
        "        if len(summary) > 1000:\n",
        "            # Try to cut at last sentence\n",
        "            last_period = summary[:1000].rfind('.')\n",
        "            if last_period > 500:\n",
        "                summary = summary[:last_period + 1]\n",
        "\n",
        "        return summary\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Generation error: {e}\")\n",
        "        return f\"Error generating summary: {str(e)[:100]}\"\n",
        "\n",
        "# MAIN PROCESSING LOOP\n",
        "print(\"\\nüöÄ Starting generation...\\n\")\n",
        "print(\"=\" * 60)\n",
        "sys.stdout.flush()\n",
        "\n",
        "for i, readme_data in enumerate(remaining):\n",
        "    try:\n",
        "        print(f\"\\n[{i+1}/{len(remaining)}] Processing: {readme_data.get('repo_name', 'Unknown')}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Generate\n",
        "        summary = generate_summary(readme_data['readme'])\n",
        "\n",
        "        # Save result\n",
        "        readme_data['summary'] = summary\n",
        "        processed_summaries.append(readme_data)\n",
        "\n",
        "        # Show FULL summary for first 5, then preview for rest\n",
        "        if i < 5:\n",
        "            print(f\"   ‚úì FULL: {summary}\")\n",
        "        else:\n",
        "            preview = summary[:100] + \"...\" if len(summary) > 100 else summary\n",
        "            print(f\"   ‚úì {preview}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Checkpoint\n",
        "        if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
        "            with open(CHECKPOINT_FILE, 'w') as f:\n",
        "                json.dump(processed_summaries, f, indent=2)\n",
        "            print(f\"\\nüíæ CHECKPOINT SAVED: {len(processed_summaries)} summaries\")\n",
        "            print(\"=\" * 60)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå FAILED {readme_data.get('repo_name', 'Unknown')}: {e}\")\n",
        "        sys.stdout.flush()\n",
        "        continue\n",
        "\n",
        "# FINAL SAVE\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üíæ Saving final results...\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "with open(CHECKPOINT_FILE, 'w') as f:\n",
        "    json.dump(processed_summaries, f, indent=2)\n",
        "\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    json.dump(processed_summaries, f, indent=2)\n",
        "\n",
        "print(f\"\"\"\n",
        "‚ú® COMPLETE! ‚ú®\n",
        "\n",
        "üìä Stats:\n",
        "   Total: {len(processed_summaries)} summaries\n",
        "   Checkpoint: {CHECKPOINT_FILE}\n",
        "   Final: {OUTPUT_FILE}\n",
        "\n",
        "üéØ Next: Download from Google Drive and train your model!\n",
        "\"\"\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# PREVIEW\n",
        "print(\"\\nüìã Sample summaries:\")\n",
        "for i, item in enumerate(processed_summaries[:3]):\n",
        "    print(f\"\\n{i+1}. {item.get('repo_name', 'Unknown')} ({item.get('stars', 0)} ‚≠ê)\")\n",
        "    print(f\"   {item['summary']}\")\n",
        "sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "unIvygqcFwyc"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import forward_ad\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import  dataclass\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#config\n",
        "@dataclass\n",
        "class config:\n",
        "  vocab_size: int  #unique words\n",
        "  block_size: int  #how far back(context) it can see, memory/ how many tokens back\n",
        "  n_layer: int   # stacked blocks, more layers more reasoning more train time\n",
        "  n_head: int   # attentions per layer, how many \"eyes\" looking for a new pattern\n",
        "  n_embd: int   #size of vector for each token\n",
        "  dropout: float  #prevents overfitting by stopping random paths\n",
        "  pad_token_id: int\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "self attention: part 1 of transformer\n",
        "Q K V, query key value. helps use the two embeddings to learn diffrent meanings for words and give the diffrent vectors even if the same word\n",
        "below is theory class is optimized, it condences the prjections into one huge vector and splits. other than that its nearly identical just more efficent\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "#learnable compenets\n",
        "q_prog = nn.Linear(C, C, bias =False)\n",
        "k_prog = nn.Linear(C, C, bias =False)\n",
        "v_prog = nn.Linear(C, C, bias =False)\n",
        "\n",
        "#weights\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "\n",
        "#preform projection\n",
        "q = q_prog(x)\n",
        "k = k_prog(x)\n",
        "v = v_prog(x)\n",
        "\n",
        "scores = q @ k.transpose(-2,-1)\n",
        "print(\"scores\",scores)\n",
        "\n",
        "\n",
        "\n",
        "Attention(Q,K,V)=softmax(‚ÄãQK^‚ä§/dk‚Äã‚Äã)V\n",
        "\n",
        "d_k = k.size(-1)#last dimesion of\n",
        "scaled_scores = scores / math.sqrt(d_k)\n",
        "attention_weights = F.softmax(scaled_scores, dim=1)\n",
        "print(\"scaled scores\", scaled_scores)\n",
        "print(\"scaled scores -> percentages\", attention_weights)\n",
        "\n",
        "# aggreation Last part of attention!\n",
        "output = attention_weights @ v\n",
        "print(\"output!:\",output)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Core logic for MultiHead\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config :config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False) # Fuzed layer = more efficent\n",
        "    self.attn_drop = nn.Dropout(config.dropout)\n",
        "    self.register_buffer( # part of causal masking\n",
        "        \"bias\",# buffer name\n",
        "        torch.tril(torch.ones(config.block_size,config.block_size))\n",
        "        .view(1,1, config.block_size, config.block_size)\n",
        "    )\n",
        "\n",
        "    self.c_proj = nn.Linear(config.n_embd,config.n_embd)\n",
        "\n",
        "  def forward(self, x,pad_mask=None):\n",
        "    B, T, C = x.size()\n",
        "    head_dim = C // self.n_head\n",
        "\n",
        "    # project once -> split\n",
        "    qkv = self.c_attn(x)\n",
        "    q, k, v = qkv.split(C, dim=2)\n",
        "\n",
        "    # reshape into heads\n",
        "    q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "    # attention\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_dim))\n",
        "    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\")) # prevents it from seeing future tokens\n",
        "\n",
        "    # Prevent attending to padding tokens (BEFORE softmax now)\n",
        "    if pad_mask is not None:\n",
        "      att = att.masked_fill(\n",
        "          pad_mask[:, None, None, :T] == 0,\n",
        "          float(\"-inf\")\n",
        "      )\n",
        "\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    att = self.attn_drop(att)\n",
        "\n",
        "    # aggregate :3\n",
        "    y = att @ v\n",
        "\n",
        "    # merge heads\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "    # final projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config :config):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(config.n_embd, 4 * config.n_embd) #expands dimestions, think of it as more room to think / combining features\n",
        "    self.proj = nn.Linear(4 * config.n_embd, config.n_embd) # condenses back so it can be added back to attetion\n",
        "    self.drop = nn.Dropout(config.dropout) #refer to config\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = F.gelu(x) # makes x nonlinear so fc and proj dont just merge into one straight line\n",
        "    x =self.proj(x)\n",
        "    x = self.drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module): #residual connection\n",
        "  def __init__(self, config : config): #litterly just does f(x) + x instead of f(x) so mlp dosesnt relearn it takes the learned/trained data and keeps it\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x, pad_mask=None):  # ‚Üê Added pad_mask parameter\n",
        "    # focus (the \"+\")\n",
        "    x = x + self.attn(self.ln_1(x), pad_mask=pad_mask)  # ‚Üê Pass mask to attention\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d9MJ7SDf9p3Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NLP(nn.Module):\n",
        "  def __init__(self, config: config):\n",
        "    super().__init__()\n",
        "    # Input\n",
        "    self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "    self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
        "    self.drop = nn.Dropout(config.dropout)\n",
        "    self.config = config\n",
        "    self.pad_token_id = config.pad_token_id\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Processing, makes a stack/block / LAYER for deeper understanding\n",
        "    # Data flows through sequncesnsy so more refined/better understanding\n",
        "    self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "\n",
        "    #output layers\n",
        "    self.ln_f = nn.LayerNorm(config.n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False) #language model head, parrel prediction(linear) makes raw score for each possible next token , good for training, and throws away the\n",
        "    #rest(all but last vector) if not traning\n",
        "    # Above makes raw score for each possible next token\n",
        "\n",
        "\n",
        "    self.lm_head.weight = self.wte.weight\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx, targets= None): #idx = input targets = inputs shifter one pos to left\n",
        "    B, T = idx.size()\n",
        "\n",
        "    assert T <= self.config.block_size, f\"Sequence length {T} exceeds block_size {self.config.block_size}\"\n",
        "\n",
        "    tok_emb = self.wte(idx)\n",
        "    tok_emb[idx == self.pad_token_id] = 0\n",
        "    pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "    pos_emb = self.wpe(pos)\n",
        "    x = self.drop(tok_emb + pos_emb)\n",
        "\n",
        "    pad_mask = (idx != self.pad_token_id).float()  # ‚Üê Mask: 1 for real tokens, 0 for padding\n",
        "\n",
        "    # Process through transformer blocks\n",
        "    for block in self.h:\n",
        "        x = block(x, pad_mask=pad_mask)  # ‚Üê Pass mask through each block\n",
        "\n",
        "    # Final layer norm\n",
        "    x = self.ln_f(x)\n",
        "\n",
        "    # Output logits\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "\n",
        "    loss = None\n",
        "\n",
        "    if targets is not None:\n",
        "      # Shift for training\n",
        "      logits_shifted = logits[:, :-1, :]\n",
        "      targets_shifted = targets[:, 1:]\n",
        "\n",
        "      loss = F.cross_entropy(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        targets.reshape(-1),\n",
        "        ignore_index=-100\n",
        "    )\n",
        "\n",
        "      print(f\"    Computed loss: {loss.item():.6f}\")\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def summarize(self, idx, max_new_tokens, temperature = 0.7, top_k =50): # <1 rare words more likely, >1 more common words\n",
        "    #uses max new tokens to create\n",
        "    for _ in range(max_new_tokens):\n",
        "      #crop if too long\n",
        "        if idx.size(1) > self.config.block_size:\n",
        "            idx_cond = idx[:, -self.config.block_size:]\n",
        "        else:\n",
        "            idx_cond = idx\n",
        "\n",
        "        logits, _ = self(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        #words used top X words\n",
        "        if top_k > 0:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHZ3z6SdsnYA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7-PUEltGnIF5"
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "# FULL TRAINING BLOCK\n",
        "# =======================\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import joblib\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2TokenizerFast\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "# -------- Drive --------\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "# -------- Device --------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------- Paths --------\n",
        "DATA_PATH = \"/content/drive/MyDrive/readme_training/summaries_clean.json\"#summaries_final.json\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/readme_model_state.pt\"\n",
        "TOKENIZER_SAVE_PATH = \"/content/drive/MyDrive/readme_tokenizer.joblib\"\n",
        "\n",
        "# -------- Tokenizer --------\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"pad_token\": \"<|pad|>\",\n",
        "    \"additional_special_tokens\": [\"<|sep|>\"]\n",
        "})\n",
        "\n",
        "PAD_ID = tokenizer.pad_token_id\n",
        "SEP_ID = tokenizer.convert_tokens_to_ids(\"<|sep|>\")\n",
        "\n",
        "cfg = config(\n",
        "    vocab_size=len(tokenizer),\n",
        "    block_size=256,\n",
        "    n_layer=6,\n",
        "    n_head=8,\n",
        "    n_embd=512,\n",
        "    dropout=0.1,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "model = NLP(cfg).to(DEVICE)\n",
        "\n",
        "# -------- Resize embeddings --------\n",
        "model.wte = torch.nn.Embedding(len(tokenizer), model.config.n_embd)\n",
        "model.lm_head.weight = model.wte.weight\n",
        "model.pad_token_id = PAD_ID\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# -------- Dataset --------\n",
        "class ReadmeSummaryDataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, block_size, min_summary_tokens=20):\n",
        "        with open(path, \"r\") as f:\n",
        "            raw_data = json.load(f)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "        self.data = []\n",
        "\n",
        "        dropped = 0\n",
        "\n",
        "        for item in raw_data:\n",
        "            readme = item.get(\"readme\", \"\").strip()\n",
        "            summary = item.get(\"summary\", \"\").strip()\n",
        "\n",
        "            if not readme or not summary:\n",
        "                dropped += 1\n",
        "                continue\n",
        "\n",
        "            readme_ids = tokenizer.encode(readme)\n",
        "            summary_ids = tokenizer.encode(summary)\n",
        "\n",
        "            if len(summary_ids) < min_summary_tokens:\n",
        "                dropped += 1\n",
        "                continue\n",
        "\n",
        "            # ensure summary actually fits after SEP\n",
        "            if len(readme_ids) + 1 >= block_size:\n",
        "                dropped += 1\n",
        "                continue\n",
        "\n",
        "            self.data.append(item)\n",
        "\n",
        "        print(f\"üßπ Dataset cleanup:\")\n",
        "        print(f\"   Loaded: {len(raw_data)}\")\n",
        "        print(f\"   Kept:   {len(self.data)}\")\n",
        "        print(f\"   Dropped:{dropped}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      item = self.data[idx]\n",
        "\n",
        "      readme_ids = self.tokenizer.encode(item[\"readme\"])\n",
        "      summary_ids = self.tokenizer.encode(item[\"summary\"])\n",
        "\n",
        "      # Input: README <SEP> SUMMARY\n",
        "      input_ids = readme_ids + [SEP_ID] + summary_ids\n",
        "\n",
        "      # Target: Ignore README, predict SUMMARY\n",
        "      # Use -100 (PyTorch's ignore_index default) instead of PAD_ID\n",
        "      targets = [-100] * len(readme_ids) + [-100] + summary_ids\n",
        "\n",
        "      # Truncate\n",
        "      input_ids = input_ids[:self.block_size]\n",
        "      targets = targets[:self.block_size]\n",
        "\n",
        "      # Pad\n",
        "      pad_len = self.block_size - len(input_ids)\n",
        "      if pad_len > 0:\n",
        "          input_ids += [PAD_ID] * pad_len\n",
        "          targets += [-100] * pad_len  # ‚Üê Use -100, not PAD_ID\n",
        "\n",
        "      return (\n",
        "          torch.tensor(input_ids, dtype=torch.long),\n",
        "          torch.tensor(targets, dtype=torch.long),\n",
        "      )\n",
        "\n",
        "\n",
        "# -------- DataLoader --------\n",
        "dataset = ReadmeSummaryDataset(\n",
        "    DATA_PATH,\n",
        "    tokenizer,\n",
        "    model.config.block_size\n",
        ")\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# -------- Optimizer --------\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=5e-5,\n",
        "    betas=(0.9, 0.95),\n",
        "    weight_decay=0.1\n",
        ")\n",
        "\n",
        "# -------- Training --------\n",
        "model.train()\n",
        "EPOCHS = 3\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
        "    for idx, targets in pbar:\n",
        "        idx = idx.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "\n",
        "        logits, loss = model(idx, targets)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "def evaluate(model, tokenizer, path, block_size): #adding eval to fix supervised training issues\n",
        "    model.eval()\n",
        "    dataset = ReadmeSummaryDataset(path, tokenizer, block_size)\n",
        "    loader = DataLoader(dataset, batch_size=4)\n",
        "\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, targets in loader:\n",
        "            idx = idx.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "\n",
        "            _, loss = model(idx, targets)\n",
        "            total_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "    model.train()\n",
        "    return total_loss / max(steps, 1)\n",
        "\n",
        "\n",
        "EVAL_PATH = \"/content/drive/MyDrive/readme_training/eval_samples.json\"\n",
        "\n",
        "if os.path.exists(EVAL_PATH):\n",
        "    eval_loss = evaluate(model, tokenizer, EVAL_PATH, model.config.block_size)\n",
        "    print(f\"üß™ Eval loss: {eval_loss:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No eval file found, skipping evaluation\")\n",
        "\n",
        "\n",
        "\n",
        "# -------- Save model + tokenizer separately --------\n",
        "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "joblib.dump(tokenizer, TOKENIZER_SAVE_PATH)\n",
        "\n",
        "print(\"‚úÖ Training complete\")\n",
        "print(f\"‚úÖ Model saved to {MODEL_SAVE_PATH}\")\n",
        "print(f\"‚úÖ Tokenizer saved to {TOKENIZER_SAVE_PATH}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE-8sVkh57m8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FINAL_MODEL_PATH = '/content/drive/MyDrive/readme_model_state.pt'\n",
        "TOKENIZER_SAVE_PATH = \"/content/drive/MyDrive/readme_tokenizer.joblib\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = joblib.load(TOKENIZER_SAVE_PATH)\n",
        "\n",
        "cfg = config(\n",
        "    vocab_size=len(tokenizer)\n",
        "    block_size=256,\n",
        "    n_layer=6,\n",
        "    n_head=8,\n",
        "    n_embd=512,\n",
        "    dropout=0.0,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "model = NLP(cfg).to(DEVICE)\n",
        "checkpoint = torch.load(FINAL_MODEL_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Test README\n",
        "test_readme = \"\"\"\n",
        " React UI Components\n",
        "\n",
        "A comprehensive React component library for building modern web applications with ease.\n",
        "\n",
        "Features\n",
        "\n",
        "Pre-built Components: Includes buttons, forms, modals, tooltips, and navigation components\n",
        "TypeScript Support: Fully typed components with IntelliSense support\n",
        "Dark Mode: Built-in theming system with automatic dark mode detection\n",
        "Responsive Design: Mobile-first components that adapt to any screen size\n",
        "Accessibility: WCAG 2.1 compliant with proper ARIA labels\n",
        "\n",
        "Installation\n",
        "\n",
        "\n",
        " Quick Start\n",
        "\n",
        "\n",
        "How It Works\n",
        "\n",
        "The library uses React hooks and context API for state management. Each component is built with styled-components for CSS-in-JS styling. The theming system uses CSS variables that can be toggled via a ThemeProvider wrapper. All components are tree-shakeable to minimize bundle size.\n",
        "\n",
        "Documentation\n",
        "\n",
        "Visit our docs at https://docs.example.com\n",
        "\"\"\"\n",
        "\n",
        "prompt_ids = tokenizer.encode(test_readme) + [SEP_ID]\n",
        "tokens = torch.tensor([prompt_ids]).to(DEVICE)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    generated = model.summarize(tokens, max_new_tokens=100, temperature=0.7, top_k=50)\n",
        "\n",
        "# Decode only the generated part (after SEP)\n",
        "result = tokenizer.decode(generated[0][len(prompt_ids):], skip_special_tokens=True)\n",
        "print(f\"Summary: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qbbtzng2n0Xc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOe1QqnpaDf0UQgXOi2iiV7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}