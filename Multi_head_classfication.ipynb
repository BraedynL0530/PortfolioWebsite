{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BraedynL0530/PortfolioWebsite/blob/master/Multi_head_classfication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "unIvygqcFwyc"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import forward_ad\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import  dataclass\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#config\n",
        "@dataclass\n",
        "class config:\n",
        "  vocab_size: int  #unique words\n",
        "  block_size: int  #how far back(context) it can see, memory/ how many tokens back\n",
        "  n_layer: int   # stacked blocks, more layers more reasoning more train time\n",
        "  n_head: int   # attentions per layer, how many \"eyes\" looking for a new pattern\n",
        "  n_embd: int   #size of vector for each token\n",
        "  dropout: float  #prevents overfitting by stopping random paths\n",
        "  pad_token_id: int\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "self attention: part 1 of transformer\n",
        "Q K V, query key value. helps use the two embeddings to learn diffrent meanings for words and give the diffrent vectors even if the same word\n",
        "below is theory class is optimized, it condences the prjections into one huge vector and splits. other than that its nearly identical just more efficent\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "#learnable compenets\n",
        "q_prog = nn.Linear(C, C, bias =False)\n",
        "k_prog = nn.Linear(C, C, bias =False)\n",
        "v_prog = nn.Linear(C, C, bias =False)\n",
        "\n",
        "#weights\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "q_prog.weight.data = torch.randn(C,C)\n",
        "\n",
        "#preform projection\n",
        "q = q_prog(x)\n",
        "k = k_prog(x)\n",
        "v = v_prog(x)\n",
        "\n",
        "scores = q @ k.transpose(-2,-1)\n",
        "print(\"scores\",scores)\n",
        "\n",
        "\n",
        "\n",
        "Attention(Q,K,V)=softmax(â€‹QK^âŠ¤/dkâ€‹â€‹)V\n",
        "\n",
        "d_k = k.size(-1)#last dimesion of\n",
        "scaled_scores = scores / math.sqrt(d_k)\n",
        "attention_weights = F.softmax(scaled_scores, dim=1)\n",
        "print(\"scaled scores\", scaled_scores)\n",
        "print(\"scaled scores -> percentages\", attention_weights)\n",
        "\n",
        "# aggreation Last part of attention!\n",
        "output = attention_weights @ v\n",
        "print(\"output!:\",output)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Core logic for MultiHead\n",
        "class CausalSelfAttention(nn.Module): #no longer casual masking, bi directional\n",
        "  def __init__(self, config :config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False) # Fuzed layer = more efficent\n",
        "    self.attn_drop = nn.Dropout(config.dropout)\n",
        "    self.register_buffer( # part of causal masking\n",
        "        \"bias\",# buffer name\n",
        "        torch.tril(torch.ones(config.block_size,config.block_size))\n",
        "        .view(1,1, config.block_size, config.block_size)\n",
        "    )\n",
        "\n",
        "    self.c_proj = nn.Linear(config.n_embd,config.n_embd)\n",
        "\n",
        "  def forward(self, x,pad_mask=None):\n",
        "    B, T, C = x.size()\n",
        "    head_dim = C // self.n_head\n",
        "\n",
        "    # project once -> split\n",
        "    qkv = self.c_attn(x)\n",
        "    q, k, v = qkv.split(C, dim=2)\n",
        "\n",
        "    # reshape into heads\n",
        "    q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
        "\n",
        "    # attention\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(head_dim))\n",
        "    #att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\")) # prevents it from seeing future tokens. Removed for bidirectional\n",
        "\n",
        "    # Prevent attending to padding tokens (BEFORE softmax now)\n",
        "    if pad_mask is not None:\n",
        "      att = att.masked_fill(\n",
        "          pad_mask[:, None, None, :T] == 0,\n",
        "          float(\"-inf\")\n",
        "      )\n",
        "\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    att = self.attn_drop(att)\n",
        "\n",
        "    # aggregate :3\n",
        "    y = att @ v\n",
        "\n",
        "    # merge heads\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "    # final projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config :config):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(config.n_embd, 4 * config.n_embd) #expands dimestions, think of it as more room to think / combining features\n",
        "    self.proj = nn.Linear(4 * config.n_embd, config.n_embd) # condenses back so it can be added back to attetion\n",
        "    self.drop = nn.Dropout(config.dropout) #refer to config\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = F.gelu(x) # makes x nonlinear so fc and proj dont just merge into one straight line\n",
        "    x =self.proj(x)\n",
        "    x = self.drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module): #residual connection\n",
        "  def __init__(self, config : config): #litterly just does f(x) + x instead of f(x) so mlp dosesnt relearn it takes the learned/trained data and keeps it\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x, pad_mask=None):  # â† Added pad_mask parameter\n",
        "    # focus (the \"+\")\n",
        "    x = x + self.attn(self.ln_1(x), pad_mask=pad_mask)\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d9MJ7SDf9p3Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NLP(nn.Module):\n",
        "  def __init__(self, config: config):\n",
        "    super().__init__()\n",
        "    # Input\n",
        "    self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "    self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
        "    self.drop = nn.Dropout(config.dropout)\n",
        "    self.config = config\n",
        "    self.pad_token_id = config.pad_token_id\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Processing, makes a stack/block / LAYER for deeper understanding\n",
        "    # Data flows through sequncesnsy so more refined/better understanding\n",
        "    self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "\n",
        "    #output layers\n",
        "    self.ln_f = nn.LayerNorm(config.n_embd) # final layer norm\n",
        "    d_model=config.n_embd\n",
        "    num_genres = 0#temp till i decied\n",
        "    num_tools = 0 #also temp\n",
        "    num_purposes = 0 #yk why\n",
        "\n",
        "    self.token_head = nn.Linear(d_model,num_tools) #seq label\n",
        "\n",
        "    self.pool_vector = nn.Linear(d_model,1) #attention pooling\n",
        "\n",
        "    # classification heads\n",
        "    self.genre_head = nn.Linear(config.n_embd, num_genres)\n",
        "    self.purpose_head = nn.Linear(config.n_embd, num_purposes)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "  def attention_pool(self, x, pad_mask):\n",
        "        \"\"\"\n",
        "        Weighted pooling using attention\n",
        "        x: [batch, seq_len, embd]\n",
        "        pad_mask: [batch, seq_len]\n",
        "        \"\"\"\n",
        "        # Compute attention scores for each token\n",
        "        attn_scores = self.pool_attention(x).squeeze(-1)  # [batch, seq_len]\n",
        "\n",
        "        # Mask out padding tokens\n",
        "        attn_scores = attn_scores.masked_fill(pad_mask == 0, float('-inf'))\n",
        "\n",
        "        # Softmax to get weights\n",
        "        attn_weights = F.softmax(attn_scores, dim=1).unsqueeze(-1)  # [batch, seq_len, 1]\n",
        "\n",
        "        # Weighted sum\n",
        "        pooled = (x * attn_weights).sum(dim=1)  # [batch, embd]\n",
        "\n",
        "        return pooled\n",
        "\n",
        "  def forward(self, idx): #idx = input targets = inputs shifter one pos to left\n",
        "    B, T = idx.size()\n",
        "\n",
        "\n",
        "    tok_emb = self.wte(idx)\n",
        "    tok_emb[idx == self.pad_token_id] = 0\n",
        "    pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "    pos_emb = self.wpe(pos)\n",
        "    x = self.drop(tok_emb + pos_emb)\n",
        "\n",
        "    pad_mask = (idx != self.pad_token_id).float()  # â† Mask: 1 for real tokens, 0 for padding\n",
        "\n",
        "    # Process through transformer blocks\n",
        "    for block in self.h:\n",
        "        x = block(x, pad_mask=pad_mask)  # â† Pass mask through each block\n",
        "\n",
        "    # Final layer norm\n",
        "    x = self.ln_f(x)\n",
        "\n",
        "    token_logits = self.token_head(x)\n",
        "\n",
        "    doc_rep = self.attention_pool(x,pad_mask)\n",
        "    genre_logits = self.genre_head(doc_rep)\n",
        "    purpose_logits = self.purpose_head(doc_rep)\n",
        "\n",
        "    return token_logits, genre_logits, purpose_logits\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data Cleaning Utilities\n",
        "========================\n",
        "\n",
        "Cleans messy README text before feeding to model.\n",
        "Handles: Unicode, emojis, special chars, formatting artifacts\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def clean_readme(text):\n",
        "    \"\"\"\n",
        "    Clean README text for better model training.\n",
        "\n",
        "    Handles:\n",
        "    - Remove excessive whitespace/newlines\n",
        "    - Strip emojis (they don't help classification)\n",
        "    - Normalize unicode\n",
        "    - Remove code blocks (they're noise)\n",
        "    - Keep only ASCII + common punctuation\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove code blocks (they're just noise for classification)\n",
        "    text = re.sub(r'```[\\s\\S]*?```', '', text)\n",
        "    text = re.sub(r'`[^`]+`', '', text)\n",
        "\n",
        "    # Remove markdown links but keep text: [text](url) -> text\n",
        "    text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
        "\n",
        "    # Remove image markdown: ![alt](url)\n",
        "    text = re.sub(r'!\\[[^\\]]*\\]\\([^\\)]+\\)', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "\n",
        "    # Normalize unicode (convert fancy chars to ASCII equivalents)\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    # Remove emojis and other unicode symbols\n",
        "    # Keep only: letters, numbers, basic punctuation, spaces\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\:\\;\\-\\(\\)\\[\\]\\/\\#\\@\\+\\=\\&]', ' ', text)\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove tabs\n",
        "    text = text.replace('\\t', ' ')\n",
        "\n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_tools_from_text(text):\n",
        "    \"\"\"\n",
        "    Try to extract tool names from README using patterns.\n",
        "    This is weak supervision - not perfect but helpful.\n",
        "\n",
        "    Common patterns:\n",
        "    - \"Built with X, Y, Z\"\n",
        "    - \"Technologies: X, Y, Z\"\n",
        "    - \"Uses: X\"\n",
        "    \"\"\"\n",
        "\n",
        "    tools = set()\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Pattern 1: \"Built with/using X\"\n",
        "    built_with = re.findall(r'built (?:with|using)[\\s:]+([^\\n\\.\\!]{5,100})', text_lower)\n",
        "    for match in built_with:\n",
        "        # Split on commas, \"and\", slashes\n",
        "        parts = re.split(r'[,/]|\\sand\\s', match)\n",
        "        tools.update([p.strip() for p in parts if p.strip()])\n",
        "\n",
        "    # Pattern 2: \"Technologies: X, Y, Z\"\n",
        "    tech_section = re.findall(r'technolog(?:ies|y)[\\s:]+([^\\n\\.\\!]{5,100})', text_lower)\n",
        "    for match in tech_section:\n",
        "        parts = re.split(r'[,/]|\\sand\\s', match)\n",
        "        tools.update([p.strip() for p in parts if p.strip()])\n",
        "\n",
        "    # Pattern 3: \"Uses: X\"\n",
        "    uses_section = re.findall(r'uses[\\s:]+([^\\n\\.\\!]{5,100})', text_lower)\n",
        "    for match in uses_section:\n",
        "        parts = re.split(r'[,/]|\\sand\\s', match)\n",
        "        tools.update([p.strip() for p in parts if p.strip()])\n",
        "\n",
        "    return list(tools)\n",
        "\n",
        "def validate_labels(tools, genres, purposes, tool_vocab, genre_vocab, purpose_vocab):\n",
        "    \"\"\"\n",
        "    Check if labels are valid (exist in vocabulary).\n",
        "    Warns about typos/unknown labels.\n",
        "    \"\"\"\n",
        "\n",
        "    invalid = []\n",
        "\n",
        "    for tool in tools:\n",
        "        if tool not in tool_vocab:\n",
        "            invalid.append(f\"Unknown tool: '{tool}'\")\n",
        "\n",
        "    for genre in genres:\n",
        "        if genre not in genre_vocab:\n",
        "            invalid.append(f\"Unknown genre: '{genre}'\")\n",
        "\n",
        "    for purpose in purposes:\n",
        "        if purpose not in purpose_vocab:\n",
        "            invalid.append(f\"Unknown purpose: '{purpose}'\")\n",
        "\n",
        "    return invalid\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE IN TRAINING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_training_data(data_path, output_path):\n",
        "    \"\"\"\n",
        "    Clean all READMEs in training data.\n",
        "\n",
        "    Before:\n",
        "    {\n",
        "        \"readme\": \"Apns4j\\n\\nLicense\\n\\nBuild Status\\n\\n...\"\n",
        "    }\n",
        "\n",
        "    After:\n",
        "    {\n",
        "        \"readme\": \"Apns4j License Build Status Quick start...\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    import json\n",
        "\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    cleaned_count = 0\n",
        "\n",
        "    for item in data:\n",
        "        if 'readme' in item:\n",
        "            original = item['readme']\n",
        "            cleaned = clean_readme(original)\n",
        "\n",
        "            if len(cleaned) < 50:  # Too short after cleaning\n",
        "                print(f\"âš ï¸  '{item.get('name', 'Unknown')}' README too short after cleaning\")\n",
        "\n",
        "            item['readme'] = cleaned\n",
        "            cleaned_count += 1\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"âœ… Cleaned {cleaned_count} READMEs\")\n",
        "    print(f\"ðŸ’¾ Saved to {output_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# GOOGLE COLAB SETUP\n",
        "# ============================================================================\n",
        "\n",
        "def setup_colab_training():\n",
        "    \"\"\"\n",
        "    Complete setup for training in Google Colab.\n",
        "\n",
        "    Run this first in Colab to mount Drive and install packages.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ðŸ“¦ Setting up Colab environment...\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Install packages\n",
        "    print(\"Installing packages...\")\n",
        "    import subprocess\n",
        "    subprocess.run(['pip', 'install', '-q', 'transformers', 'torch'])\n",
        "\n",
        "    # Create directories\n",
        "    import os\n",
        "    base_dir = '/content/drive/MyDrive/portfolio_classifier'\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    os.makedirs(f'{base_dir}/data', exist_ok=True)\n",
        "    os.makedirs(f'{base_dir}/models', exist_ok=True)\n",
        "\n",
        "    print(f\"âœ… Setup complete!\")\n",
        "    print(f\"ðŸ“ Working directory: {base_dir}\")\n",
        "    print()\n",
        "    print(\"Next steps:\")\n",
        "    print(\"1. Upload labeled_data.json to /content/drive/MyDrive/portfolio_classifier/data/\")\n",
        "    print(\"2. Upload your NLP model code\")\n",
        "    print(\"3. Run training script\")\n",
        "\n",
        "    return base_dir\n",
        "\n",
        "# ============================================================================\n",
        "# COLAB TRAINING SCRIPT\n",
        "# ============================================================================\n",
        "\n",
        "COLAB_TRAINING_SCRIPT = \"\"\"\n",
        "# Google Colab Training Script\n",
        "# =============================\n",
        "# Run this in a Colab notebook cell\n",
        "\n",
        "# 1. Setup\n",
        "from data_cleaning import setup_colab_training, preprocess_training_data, clean_readme\n",
        "base_dir = setup_colab_training()\n",
        "\n",
        "# 2. Clean your data\n",
        "preprocess_training_data(\n",
        "    f'{base_dir}/data/labeled_data.json',\n",
        "    f'{base_dir}/data/labeled_data_clean.json'\n",
        ")\n",
        "\n",
        "# 3. Load your model (upload NLP.py to Colab first)\n",
        "from NLP import NLP, config\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 4. Initialize model\n",
        "model_config = config(\n",
        "    vocab_size=50257,\n",
        "    block_size=512,\n",
        "    n_layer=4,\n",
        "    n_head=8,\n",
        "    n_embd=256,\n",
        "    dropout=0.1,\n",
        "    pad_token_id=50256\n",
        ")\n",
        "\n",
        "model = NLP(\n",
        "    model_config,\n",
        "    num_tools=30,  # Adjust to your vocab sizes\n",
        "    num_genres=10,\n",
        "    num_purposes=15\n",
        ").to(device)\n",
        "\n",
        "# 5. Train\n",
        "from train_portfolio_classifier import RepoDataset, train_epoch\n",
        "from transformers import GPT2TokenizerFast\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "dataset = RepoDataset(\n",
        "    f'{base_dir}/data/labeled_data_clean.json',\n",
        "    tokenizer,\n",
        "    max_len=512,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss = train_epoch(model, loader, optimizer, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/10: Loss = {loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint every 3 epochs\n",
        "    if (epoch + 1) % 3 == 0:\n",
        "        torch.save(\n",
        "            model.state_dict(),\n",
        "            f'{base_dir}/models/checkpoint_epoch_{epoch+1}.pt'\n",
        "        )\n",
        "        print(f\"ðŸ’¾ Saved checkpoint\")\n",
        "\n",
        "# 6. Save final model\n",
        "torch.save(model.state_dict(), f'{base_dir}/models/portfolio_classifier_final.pt')\n",
        "print(\"âœ… Training complete!\")\n",
        "print(f\"ðŸ“ Model saved to: {base_dir}/models/portfolio_classifier_final.pt\")\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Data Cleaning Utilities\")\n",
        "    print(\"=\" * 50)\n",
        "    print()\n",
        "    print(\"Functions available:\")\n",
        "    print(\"  clean_readme(text) - Clean messy README text\")\n",
        "    print(\"  extract_tools_from_text(text) - Auto-detect tools\")\n",
        "    print(\"  validate_labels(...) - Check label validity\")\n",
        "    print(\"  preprocess_training_data(...) - Clean entire dataset\")\n",
        "    print(\"  setup_colab_training() - Setup Google Colab environment\")\n",
        "    print()\n",
        "    print(\"Example usage:\")\n",
        "    print()\n",
        "    print(\"# Clean single README\")\n",
        "    print(\"clean_text = clean_readme(messy_readme)\")\n",
        "    print()\n",
        "    print(\"# Clean entire dataset\")\n",
        "    print(\"preprocess_training_data('raw_data.json', 'clean_data.json')\")\n",
        "    print()\n",
        "    print(\"# For Google Colab:\")\n",
        "    print(\"base_dir = setup_colab_training()\")"
      ],
      "metadata": {
        "id": "wzbHCVmStRu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Portfolio Classifier - Google Colab Training Notebook\n",
        "# ======================================================\n",
        "# Upload this to Google Colab and run cell by cell\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: Setup Environment\n",
        "# ============================================================================\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install packages\n",
        "!pip install -q transformers torch\n",
        "\n",
        "# Create directories\n",
        "import os\n",
        "base_dir = '/content/drive/MyDrive/portfolio_classifier'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "os.makedirs(f'{base_dir}/data', exist_ok=True)\n",
        "os.makedirs(f'{base_dir}/models', exist_ok=True)\n",
        "os.makedirs(f'{base_dir}/code', exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Setup complete!\")\n",
        "print(f\"ðŸ“ Base directory: {base_dir}\")\n",
        "print()\n",
        "print(\"Next: Upload your files to Google Drive:\")\n",
        "print(f\"  - labeled_data.json â†’ {base_dir}/data/\")\n",
        "print(f\"  - NLP.py (your model) â†’ {base_dir}/code/\")\n",
        "print(f\"  - train_portfolio_classifier.py â†’ {base_dir}/code/\")\n",
        "print(f\"  - data_cleaning.py â†’ {base_dir}/code/\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: Add code to path and import\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "sys.path.append(f'{base_dir}/code')\n",
        "\n",
        "# Import your modules\n",
        "from data_cleaning import clean_readme, preprocess_training_data\n",
        "# from NLP import NLP, config  # Uncomment after uploading NLP.py\n",
        "\n",
        "print(\"âœ… Modules imported\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3: Clean Data\n",
        "# ============================================================================\n",
        "\n",
        "# Option A: Clean existing data file\n",
        "preprocess_training_data(\n",
        "    f'{base_dir}/data/labeled_data.json',\n",
        "    f'{base_dir}/data/labeled_data_clean.json'\n",
        ")\n",
        "\n",
        "# Option B: Dataset will auto-clean on load (set clean=True)\n",
        "# Just use labeled_data.json directly\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 4: Check GPU\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"ðŸ–¥ï¸  Using device: {device}\")\n",
        "\n",
        "if device == 'cuda':\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  No GPU detected! Training will be slow.\")\n",
        "    print(\"   Go to: Runtime â†’ Change runtime type â†’ GPU\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 5: Define Model Config (copy your config here)\n",
        "# ============================================================================\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int = 50257  # GPT-2 vocab size\n",
        "    block_size: int = 512\n",
        "    n_layer: int = 4         # Small model for fast training\n",
        "    n_head: int = 8\n",
        "    n_embd: int = 256\n",
        "    dropout: float = 0.1\n",
        "    pad_token_id: int = 50256\n",
        "\n",
        "# Classification vocab sizes\n",
        "NUM_TOOLS = 30\n",
        "NUM_GENRES = 10\n",
        "NUM_PURPOSES = 15\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: Load Model\n",
        "# ============================================================================\n",
        "\n",
        "# Import your NLP model (make sure NLP.py is in {base_dir}/code/)\n",
        "from NLP import NLP\n",
        "\n",
        "config = ModelConfig()\n",
        "\n",
        "model = NLP(\n",
        "    config,\n",
        "    num_tools=NUM_TOOLS,\n",
        "    num_genres=NUM_GENRES,\n",
        "    num_purposes=NUM_PURPOSES\n",
        ").to(device)\n",
        "\n",
        "print(f\"âœ… Model loaded\")\n",
        "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 7: Prepare Data\n",
        "# ============================================================================\n",
        "\n",
        "from train_portfolio_classifier import RepoDataset\n",
        "from transformers import GPT2TokenizerFast\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load dataset (with auto-cleaning)\n",
        "dataset = RepoDataset(\n",
        "    f'{base_dir}/data/labeled_data.json',  # or labeled_data_clean.json\n",
        "    tokenizer,\n",
        "    max_len=512,\n",
        "    augment=True,  # Auto-detect additional tools\n",
        "    clean=True     # Auto-clean messy text\n",
        ")\n",
        "\n",
        "# Create dataloader\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,  # Adjust based on GPU memory\n",
        "    shuffle=True,\n",
        "    pin_memory=True if device == 'cuda' else False\n",
        ")\n",
        "\n",
        "print(f\"âœ… Data loaded: {len(dataset)} examples\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 8: Training Setup\n",
        "# ============================================================================\n",
        "\n",
        "from train_portfolio_classifier import FocalLoss, train_epoch\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=3e-4,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Loss function (handles class imbalance)\n",
        "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
        "\n",
        "# Training config\n",
        "EPOCHS = 10\n",
        "SAVE_EVERY = 3  # Save checkpoint every N epochs\n",
        "\n",
        "print(\"ðŸš€ Ready to train!\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {loader.batch_size}\")\n",
        "print(f\"   Steps per epoch: {len(loader)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 9: Train Model\n",
        "# ============================================================================\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING START\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "training_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Train one epoch\n",
        "    loss = train_epoch(model, loader, optimizer, criterion, device)\n",
        "    training_losses.append(loss)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    print(f\"\\nðŸ“Š Epoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"   Loss: {loss:.4f}\")\n",
        "    print(f\"   Time: {epoch_time:.1f}s\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch + 1) % SAVE_EVERY == 0:\n",
        "        checkpoint_path = f'{base_dir}/models/checkpoint_epoch_{epoch+1}.pt'\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"   ðŸ’¾ Checkpoint saved\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 10: Save Final Model\n",
        "# ============================================================================\n",
        "\n",
        "# Save model\n",
        "final_model_path = f'{base_dir}/models/portfolio_classifier_final.pt'\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_path = f'{base_dir}/models/tokenizer'\n",
        "tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "print(f\"âœ… Model saved to: {final_model_path}\")\n",
        "print(f\"âœ… Tokenizer saved to: {tokenizer_path}\")\n",
        "print()\n",
        "print(\"ðŸ“Š Training Summary:\")\n",
        "print(f\"   Initial loss: {training_losses[0]:.4f}\")\n",
        "print(f\"   Final loss: {training_losses[-1]:.4f}\")\n",
        "print(f\"   Improvement: {training_losses[0] - training_losses[-1]:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 11: Test Inference\n",
        "# ============================================================================\n",
        "\n",
        "from train_portfolio_classifier import predict\n",
        "\n",
        "# Test on a README\n",
        "test_readme = \"\"\"\n",
        "My Awesome Project\n",
        "\n",
        "A web application built with Django and React for real-time data visualization.\n",
        "\n",
        "Tech Stack:\n",
        "- Django REST Framework\n",
        "- React with TypeScript\n",
        "- PostgreSQL\n",
        "- Redis for caching\n",
        "- Celery for background tasks\n",
        "\n",
        "Features:\n",
        "- Real-time dashboards\n",
        "- User authentication\n",
        "- API for external integrations\n",
        "\"\"\"\n",
        "\n",
        "print(\"ðŸ§ª Testing inference...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "predictions = predict(model, test_readme, tokenizer, threshold=0.5)\n",
        "\n",
        "print(\"ðŸ“¦ Test README:\")\n",
        "print(test_readme[:200] + \"...\")\n",
        "print()\n",
        "print(\"ðŸ¤– Predictions:\")\n",
        "print()\n",
        "print(\"Genres:\")\n",
        "for p in predictions['genres']:\n",
        "    print(f\"  â€¢ {p['label']} ({p['confidence']:.1%})\")\n",
        "\n",
        "print(\"\\nPurposes:\")\n",
        "for p in predictions['purposes']:\n",
        "    print(f\"  â€¢ {p['label']} ({p['confidence']:.1%})\")\n",
        "\n",
        "print(\"\\nâœ… Inference works!\")\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 12: Download Model (Optional)\n",
        "# ============================================================================\n",
        "\n",
        "# If you want to download model to local machine\n",
        "from google.colab import files\n",
        "\n",
        "# Download model\n",
        "# files.download(final_model_path)\n",
        "\n",
        "# Or just use it directly from Google Drive in your app!\n",
        "\n",
        "print(\"ðŸŽ‰ All done!\")\n",
        "print()\n",
        "print(\"Next steps:\")\n",
        "print(\"1. Test on more examples\")\n",
        "print(\"2. Use label_review_tool.py to add more training data\")\n",
        "print(\"3. Retrain with expanded dataset\")\n",
        "print(\"4. Deploy in your portfolio\")"
      ],
      "metadata": {
        "id": "sE_Pi3f02WSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Portfolio Project Classifier - Full Training Pipeline\n",
        "======================================================\n",
        "\n",
        "This script trains a multi-label classifier to extract:\n",
        "- Tools/technologies used (django, react, pytorch, etc.)\n",
        "- Project genre (web-app, dev-tools, ml, etc.)\n",
        "- Project purpose (analytics, automation, etc.)\n",
        "\n",
        "The model uses a transformer encoder (your NLP class) with separate\n",
        "classification heads for each task.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2TokenizerFast\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    # Vocabularies - START SMALL, expand as needed\n",
        "    TOOL_VOCAB = [\n",
        "        # Frontend\n",
        "        \"react\", \"vue\", \"angular\", \"svelte\", \"nextjs\", \"html\", \"css\", \"javascript\", \"typescript\",\n",
        "        # Backend\n",
        "        \"django\", \"flask\", \"fastapi\", \"express\", \"nodejs\", \"go\", \"rust\",\n",
        "        # Database\n",
        "        \"postgres\", \"mysql\", \"mongodb\", \"redis\", \"sqlite\",\n",
        "        # ML/Data\n",
        "        \"pytorch\", \"tensorflow\", \"sklearn\", \"pandas\", \"numpy\",\n",
        "        # Desktop/Mobile\n",
        "        \"pyqt5\", \"electron\", \"flutter\", \"swift\",\n",
        "        # DevOps\n",
        "        \"docker\", \"kubernetes\", \"aws\", \"gcp\",\n",
        "        # Other\n",
        "        \"python\", \"celery\"\n",
        "    ]\n",
        "\n",
        "    GENRE_VOCAB = [\n",
        "        \"web-app\", \"api\", \"cli\", \"library\", \"dev-tools\",\n",
        "        \"ml\", \"desktop\", \"mobile\", \"bot\", \"game\"\n",
        "    ]\n",
        "\n",
        "    PURPOSE_VOCAB = [\n",
        "        \"analytics\", \"monitoring\", \"automation\", \"productivity\",\n",
        "        \"visualization\", \"e-commerce\", \"social\", \"education\",\n",
        "        \"security\", \"real-time\", \"scraping\"\n",
        "    ]\n",
        "\n",
        "    # Tool synonyms for auto-labeling\n",
        "    TOOL_SYNONYMS = {\n",
        "        \"go\": [\"golang\", \"go language\"],\n",
        "        \"javascript\": [\"js\", \"node\"],\n",
        "        \"python\": [\"py\"],\n",
        "        \"typescript\": [\"ts\"],\n",
        "        \"postgres\": [\"postgresql\", \"psql\"],\n",
        "        \"react\": [\"reactjs\"],\n",
        "        \"vue\": [\"vuejs\"],\n",
        "        \"pyqt5\": [\"pyqt\", \"qt5\", \"qt\"],\n",
        "        \"sklearn\": [\"scikit-learn\", \"scikit\"],\n",
        "        \"pytorch\": [\"torch\"],\n",
        "        \"nodejs\": [\"node.js\", \"node\"],\n",
        "    }\n",
        "\n",
        "    # Model config\n",
        "    VOCAB_SIZE = 50257  # GPT-2 tokenizer size\n",
        "    BLOCK_SIZE = 512\n",
        "    N_LAYER = 4  # Start small, increase if underfitting\n",
        "    N_HEAD = 8\n",
        "    N_EMBD = 256\n",
        "    DROPOUT = 0.1\n",
        "\n",
        "    # Training config\n",
        "    BATCH_SIZE = 4\n",
        "    LEARNING_RATE = 3e-4\n",
        "    EPOCHS = 10\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Create label mappings\n",
        "tool2idx = {tool: i for i, tool in enumerate(config.TOOL_VOCAB)}\n",
        "genre2idx = {genre: i for i, genre in enumerate(config.GENRE_VOCAB)}\n",
        "purpose2idx = {purpose: i for i, purpose in enumerate(config.PURPOSE_VOCAB)}\n",
        "\n",
        "idx2tool = {i: tool for tool, i in tool2idx.items()}\n",
        "idx2genre = {i: genre for genre, i in genre2idx.items()}\n",
        "idx2purpose = {i: purpose for purpose, i in purpose2idx.items()}\n",
        "\n",
        "# ============================================================================\n",
        "# LABEL AUGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "def augment_tool_labels(readme_text, manual_labels):\n",
        "    \"\"\"\n",
        "    Automatically detect additional tools from README text using string matching.\n",
        "    This is \"weak supervision\" - not perfect but helps label faster.\n",
        "\n",
        "    Args:\n",
        "        readme_text: The full README content\n",
        "        manual_labels: List of tools you manually labeled\n",
        "\n",
        "    Returns:\n",
        "        Combined list of manual + auto-detected tools\n",
        "    \"\"\"\n",
        "    text_lower = readme_text.lower()\n",
        "    auto_labels = set(manual_labels)  # Start with manual labels\n",
        "\n",
        "    for tool, synonyms in config.TOOL_SYNONYMS.items():\n",
        "        # Check if tool name or any synonym appears in text\n",
        "        all_names = [tool] + synonyms\n",
        "        if any(name in text_lower for name in all_names):\n",
        "            auto_labels.add(tool)\n",
        "\n",
        "    return list(auto_labels)\n",
        "\n",
        "def labels_to_vector(label_list, vocab_dict, vocab_size):\n",
        "    \"\"\"\n",
        "    Convert list of string labels to binary vector.\n",
        "\n",
        "    Example:\n",
        "        labels = [\"django\", \"react\"]\n",
        "        vocab = {\"django\": 0, \"flask\": 1, \"react\": 2, ...}\n",
        "        result = [1, 0, 1, 0, 0, ...] (50-dim vector)\n",
        "    \"\"\"\n",
        "    vector = torch.zeros(vocab_size)\n",
        "    for label in label_list:\n",
        "        if label in vocab_dict:\n",
        "            vector[vocab_dict[label]] = 1\n",
        "    return vector\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET\n",
        "# ============================================================================\n",
        "\n",
        "class RepoDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for training the multi-label classifier.\n",
        "\n",
        "    Expected data format (JSON):\n",
        "    [\n",
        "        {\n",
        "            \"name\": \"Desktop Pet ML\",\n",
        "            \"readme\": \"DESKTOP PET w/ MACHINE LEARNING...\",\n",
        "            \"tools\": [\"pyqt5\", \"python\", \"sqlite\"],\n",
        "            \"genres\": [\"desktop\", \"dev-tools\"],\n",
        "            \"purposes\": [\"productivity\", \"automation\"]\n",
        "        },\n",
        "        ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path, tokenizer, max_len=512, augment=True, clean=True):\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.augment = augment\n",
        "\n",
        "        # Clean READMEs on load\n",
        "        if clean:\n",
        "            print(\"ðŸ§¹ Cleaning READMEs...\")\n",
        "            from data_cleaning import clean_readme\n",
        "            for item in self.data:\n",
        "                if 'readme' in item:\n",
        "                    item['readme'] = clean_readme(item['readme'])\n",
        "\n",
        "        print(f\"ðŸ“Š Loaded {len(self.data)} examples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Tokenize README (truncate if too long)\n",
        "        readme = item['readme']\n",
        "        tokens = self.tokenizer.encode(readme)[:self.max_len]\n",
        "\n",
        "        # Pad to max_len\n",
        "        if len(tokens) < self.max_len:\n",
        "            tokens += [self.tokenizer.pad_token_id] * (self.max_len - len(tokens))\n",
        "\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "        # Get labels (with optional augmentation)\n",
        "        tool_labels = item.get('tools', [])\n",
        "        if self.augment:\n",
        "            tool_labels = augment_tool_labels(readme, tool_labels)\n",
        "\n",
        "        genre_labels = item.get('genres', [])\n",
        "        purpose_labels = item.get('purposes', [])\n",
        "\n",
        "        # Convert to binary vectors\n",
        "        tools_vec = labels_to_vector(tool_labels, tool2idx, len(config.TOOL_VOCAB))\n",
        "        genres_vec = labels_to_vector(genre_labels, genre2idx, len(config.GENRE_VOCAB))\n",
        "        purposes_vec = labels_to_vector(purpose_labels, purpose2idx, len(config.PURPOSE_VOCAB))\n",
        "\n",
        "        return tokens, tools_vec, genres_vec, purposes_vec\n",
        "\n",
        "# ============================================================================\n",
        "# LOSS FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss - handles class imbalance better than BCE.\n",
        "\n",
        "    Focuses training on hard examples. If a label is rare (like \"game\"),\n",
        "    the model pays more attention to learning it.\n",
        "\n",
        "    alpha: weight for positive examples (0.25 = focus more on negatives)\n",
        "    gamma: focusing parameter (2 = strongly focus on hard examples)\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # BCE loss\n",
        "        bce = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "\n",
        "        # Probability of correct prediction\n",
        "        pt = torch.exp(-bce)\n",
        "\n",
        "        # Focal weight: (1 - pt)^gamma\n",
        "        # Easy examples (pt close to 1) get low weight\n",
        "        # Hard examples (pt close to 0) get high weight\n",
        "        focal_weight = (1 - pt) ** self.gamma\n",
        "\n",
        "        # Final loss\n",
        "        focal_loss = self.alpha * focal_weight * bce\n",
        "\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train for one epoch.\n",
        "\n",
        "    Returns:\n",
        "        Average loss for the epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for tokens, tools, genres, purposes in pbar:\n",
        "        tokens = tokens.to(device)\n",
        "        tools = tools.to(device)\n",
        "        genres = genres.to(device)\n",
        "        purposes = purposes.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        _, genre_logits, purpose_logits = model(tokens)\n",
        "\n",
        "        # For now, skip token-level (tools) - just do doc-level\n",
        "        # You can add token-level later if needed\n",
        "\n",
        "        # Compute losses\n",
        "        genre_loss = criterion(genre_logits, genres)\n",
        "        purpose_loss = criterion(purpose_logits, purposes)\n",
        "\n",
        "        total_loss_batch = genre_loss + purpose_loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        total_loss_batch.backward()\n",
        "\n",
        "        # Gradient clipping (prevents exploding gradients)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += total_loss_batch.item()\n",
        "        pbar.set_postfix(loss=f\"{total_loss_batch.item():.4f}\")\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on validation set.\n",
        "\n",
        "    Returns:\n",
        "        Average loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    for tokens, tools, genres, purposes in dataloader:\n",
        "        tokens = tokens.to(device)\n",
        "        genres = genres.to(device)\n",
        "        purposes = purposes.to(device)\n",
        "\n",
        "        _, genre_logits, purpose_logits = model(tokens)\n",
        "\n",
        "        genre_loss = criterion(genre_logits, genres)\n",
        "        purpose_loss = criterion(purpose_logits, purposes)\n",
        "\n",
        "        total_loss += (genre_loss + purpose_loss).item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# ============================================================================\n",
        "# INFERENCE\n",
        "# ============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(model, readme_text, tokenizer, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predict labels for a single README.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        readme_text: Full README content (string)\n",
        "        tokenizer: Tokenizer\n",
        "        threshold: Confidence threshold (0.5 = 50%)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with predicted labels and confidence scores\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = tokenizer.encode(readme_text)[:config.BLOCK_SIZE]\n",
        "    if len(tokens) < config.BLOCK_SIZE:\n",
        "        tokens += [tokenizer.pad_token_id] * (config.BLOCK_SIZE - len(tokens))\n",
        "\n",
        "    tokens = torch.tensor([tokens]).to(config.DEVICE)\n",
        "\n",
        "    # Forward pass\n",
        "    _, genre_logits, purpose_logits = model(tokens)\n",
        "\n",
        "    # Convert to probabilities\n",
        "    genre_probs = torch.sigmoid(genre_logits)[0]\n",
        "    purpose_probs = torch.sigmoid(purpose_logits)[0]\n",
        "\n",
        "    # Get predictions above threshold\n",
        "    predicted_genres = []\n",
        "    predicted_purposes = []\n",
        "\n",
        "    for i, prob in enumerate(genre_probs):\n",
        "        if prob > threshold:\n",
        "            predicted_genres.append({\n",
        "                \"label\": idx2genre[i],\n",
        "                \"confidence\": prob.item()\n",
        "            })\n",
        "\n",
        "    for i, prob in enumerate(purpose_probs):\n",
        "        if prob > threshold:\n",
        "            predicted_purposes.append({\n",
        "                \"label\": idx2purpose[i],\n",
        "                \"confidence\": prob.item()\n",
        "            })\n",
        "\n",
        "    return {\n",
        "        \"genres\": predicted_genres,\n",
        "        \"purposes\": predicted_purposes\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING SCRIPT\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main training function.\n",
        "\n",
        "    To use:\n",
        "    1. Create labeled_data.json with your manual labels\n",
        "    2. Run: python train_portfolio_classifier.py\n",
        "    3. Model saves to portfolio_classifier.pt\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ðŸš€ Portfolio Classifier Training\")\n",
        "    print(f\"Device: {config.DEVICE}\")\n",
        "    print(f\"Tools: {len(config.TOOL_VOCAB)}\")\n",
        "    print(f\"Genres: {len(config.GENRE_VOCAB)}\")\n",
        "    print(f\"Purposes: {len(config.PURPOSE_VOCAB)}\")\n",
        "    print()\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load your NLP model here\n",
        "    # from your_model_file import NLP, config as model_config\n",
        "    # model = NLP(model_config,\n",
        "    #             num_tools=len(config.TOOL_VOCAB),\n",
        "    #             num_genres=len(config.GENRE_VOCAB),\n",
        "    #             num_purposes=len(config.PURPOSE_VOCAB))\n",
        "    # model = model.to(config.DEVICE)\n",
        "\n",
        "    print(\"âš ï¸  Import your NLP model and uncomment the training code!\")\n",
        "    print()\n",
        "    print(\"Expected data format:\")\n",
        "    print(json.dumps({\n",
        "        \"name\": \"Example Project\",\n",
        "        \"readme\": \"Full README text here...\",\n",
        "        \"tools\": [\"django\", \"react\"],\n",
        "        \"genres\": [\"web-app\"],\n",
        "        \"purposes\": [\"analytics\"]\n",
        "    }, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XZNC5GmB2cIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNOjVkp6mMyZZCKvo0V4HDX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}